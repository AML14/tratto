{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CodeBERT Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T17:04:34.106369Z",
     "start_time": "2023-05-13T17:04:24.355099Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T16:10:37.877355Z",
     "start_time": "2023-05-15T16:10:35.247422Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import math\n",
    "import json\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from functools import reduce\n",
    "from enum import Enum\n",
    "from collections import Counter\n",
    "from torch.utils.data import ConcatDataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "from torch.nn import Module, Linear, CrossEntropyLoss\n",
    "from transformers import AutoModel\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## **Setup**\n",
    "* Definition of the dataset path to load the data\n",
    "* Connection to the GPU (or TPU) made available for free by Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T16:11:21.691041Z",
     "start_time": "2023-05-15T16:11:21.676977Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # Connect to google drive (personal account)\n",
    "    # Change the path according to the position of your dataset in your personal account\n",
    "    # The next statements require the authorization to connect the colab notebook\n",
    "    # to your personal account\n",
    "    drive.mount('/content/drive')\n",
    "    d_path = os.path.join(\"drive\", \"MyDrive\", \"Colab Notebooks\", \"asterix\")\n",
    "else:\n",
    "    # The notebook is run in local or within a server\n",
    "    # Change the path according to the position of your dataset in the machine\n",
    "    d_path = os.path.join(os.getcwd(), \"..\", \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T16:11:22.871902Z",
     "start_time": "2023-05-15T16:11:22.859942Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davidemolinelli/Documents/phd/repositories/ASTERIX/Oracle/Implementation/ml-model/src/notebooks/../../oracle'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T16:11:23.544528Z",
     "start_time": "2023-05-15T16:11:23.530866Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Connection to the gpu (if available). Otherwise, the cpu is used\n",
    "# In case the notebook runs in Google Colab, to connect the notebook to the gpu\n",
    "# go to: Runtime --> Change runtime type, and set the hardware accelerator to\n",
    "# gpu or tpu\n",
    "if torch.cuda.is_available():\n",
    "    # Set the gpu as device to perform the training\n",
    "    DEVICE = torch.device(\"cuda:0\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print(f'We will use the GPU: torch.cuda.get_device_name(0)')\n",
    "else:\n",
    "    # Set the cpu as device to perform the training\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T16:11:27.521764Z",
     "start_time": "2023-05-15T16:11:27.493732Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# 80% of the dataset will be used for training (the remaining 20% for validation)\n",
    "TRAINING_RATIO = 0.8\n",
    "# number of sequence in each batch\n",
    "BATCH_SIZE = 16\n",
    "# number of epochs to train the model\n",
    "NUM_EPOCHS = 1\n",
    "# number of steps after which computes the validation loss and accuracy\n",
    "N_STEPS = 8\n",
    "# learning rate for the gradient descent\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Loads dataset from specified `d_path`. Drops empty `label` column and fills null values with empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T16:44:19.302765Z",
     "start_time": "2023-05-15T16:44:18.831508Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "dfs = []\n",
    "\n",
    "for file_name in os.listdir(os.path.join(d_path, \"dataset\")):\n",
    "    df = pd.read_json(os.path.join(d_path, \"dataset\", file_name))\n",
    "    dfs.append(df)\n",
    "df_dataset = pd.concat(dfs)\n",
    "# drop column id (it is not relevant for training the model)\n",
    "df_dataset = df_dataset.drop(['id'], axis=1)\n",
    "# map empty cells to empty strings\n",
    "df_dataset.fillna('', inplace=True)\n",
    "# specify the type of each column in the dataset\n",
    "df_dataset = df_dataset.astype({\n",
    "    'label': 'bool',\n",
    "    'oracleId': 'int64',\n",
    "    'oracleType': 'string',\n",
    "    'projectName': 'string',\n",
    "    'packageName': 'string',\n",
    "    'className': 'string',\n",
    "    'javadocTag': 'string',\n",
    "    'methodJavadoc': 'string',\n",
    "    'methodSourceCode': 'string',\n",
    "    'classJavadoc': 'string',\n",
    "    'classSourceCode': 'string',\n",
    "    'oracleSoFar': 'string',\n",
    "    'token': 'string',\n",
    "    'tokenClass': 'string',\n",
    "    'tokenInfo': 'string'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T16:44:21.033360Z",
     "start_time": "2023-05-15T16:44:21.029431Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4368, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:05.606349Z",
     "start_time": "2023-05-15T15:46:05.599492Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# delete the oracle ids and the tgt labels from the input dataset\n",
    "df_src = df_dataset.drop(['oracleId','oracleType','label'], axis=1)\n",
    "# create a dataframe for the oracle ids (we convert the boolean values to int64)\n",
    "df_oracle_ids = df_dataset[['oracleId']]\n",
    "# create a dataframe for the target labels (the apply function convert the\n",
    "# boolean labels to 0s and 1s\n",
    "df_tgt = df_dataset[['label']].replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "Loads `codebert-base` from `AutoTokenizer`. Tokenizes inputs feature-wise and creates `DataLoader` objects for `input_ids`, `attention_masks`, and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:07.228473Z",
     "start_time": "2023-05-15T15:46:06.406493Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:07.817968Z",
     "start_time": "2023-05-15T15:46:07.304252Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# The apply function maps each row of the src dataset with multiple columns, to\n",
    "# a row with a single column containing the concatenation of the strings of each\n",
    "# original column, using a token as a separator.\n",
    "#\n",
    "# For example:\n",
    "#\n",
    "#   1. Given the first row of the dataset:\n",
    "#\n",
    "#         projectName                                 \"commons-collections4-4.1\"\n",
    "#         packageName                          \"org.apache.commons.collections4\"\n",
    "#         className                                                    \"Equator\"\n",
    "#         javadocTag                \"@return whether the two objects are equal.\"\n",
    "#         methodJavadoc      \"/**\\n     * Evaluates the two arguments for th...\"\n",
    "#         methodSourceCode                         \"boolean equate(T o1, T o2);\"\n",
    "#         classJavadoc       \"/**\\n * An equation function, which determines...\"\n",
    "#         oracleSoFar                                                         \"\"\n",
    "#         token                                                              \"(\"\n",
    "#         tokenClass                                        \"OpeningParenthesis\"\n",
    "#         tokenInfo                                                           \"\"\n",
    "#         notes                                                               \"\"\n",
    "#\n",
    "#   2. The statement gets the values of each column in an array (row.values)\n",
    "#\n",
    "#         [\"commons-collections4-4.1\",\"org.apache.commons.collections4\",...,\"\",\"\"]\n",
    "#\n",
    "#   3. The join method concatenates all the values in the array into a string,\n",
    "#      using a special token (the *cls_token*) as separator.\n",
    "#\n",
    "#         commons-collections4-4.1<s>org.apache.commons.collections4<s>...<s>OpeningParenthesis<s><s>\n",
    "#\n",
    "# The result of step (3) represents the content of the unique column of the new\n",
    "# map row. The process is repeated for each row in the src dataset.\n",
    "df_src_concat = df_src.apply(lambda row: tokenizer.cls_token.join(row.values), axis=1)\n",
    "# The pandas dataframe is transformed in a list of strings: each string is a input\n",
    "# to the model\n",
    "src_list = df_src_concat.to_numpy().tolist()\n",
    "# We also transform the oracle ids and the targets to a list\n",
    "oracle_ids_list = df_oracle_ids.to_numpy().tolist()\n",
    "tgt_numpy = df_tgt.to_numpy().ravel()\n",
    "tgt_classes = tgt_numpy.max() + 1\n",
    "one_hot_tgt = np.zeros((len(tgt_numpy), tgt_classes))\n",
    "one_hot_tgt[np.arange(len(tgt_numpy)), tgt_numpy] = 1\n",
    "tgt_list = one_hot_tgt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:07.825349Z",
     "start_time": "2023-05-15T15:46:07.820850Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper enum class: defines the types of sorting algorithm for the whole dataset.\n",
    "class BatchType(str, Enum):\n",
    "    HETEROGENEOUS = \"HETEROGENEOUS\"\n",
    "    OMOGENEOUS = \"OMOGENEOUS\"\n",
    "    RANDOM = \"RANDOM\"\n",
    "    SORTED_BY_LENGTH = \"SORTED_BY_LENGTH\"\n",
    "\n",
    "# Helper enum class: defines the types of dataset to create (training and\n",
    "# validation)\n",
    "class DatasetType(str, Enum):\n",
    "    TRAINING = \"TRAINING\"\n",
    "    VALIDATION = \"VALIDATION\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data Processor\n",
    "\n",
    "The **DataProcessor** class takes care of pre-processing the raw datasets to create the final datasets, efficiently sorted (with customizable criteria) and tokenized.\n",
    "\n",
    "The datasets generated by the **DataProcessor** class can be feed to the **Dataset** and **DataLoader** PyTorch classes to easily load the data and prepare the batches to feed the model.\n",
    "\n",
    "The tokenized datasets are sorted in a way that let the **Dataloader** to generate the final batches processing the datasets sequentially (using the **SequentialSampler** PyTorch helper class).\n",
    "\n",
    "Although already tokenized, the datasets contains elements of different size (implicitly grouped by the length of the batches, previously defined by the hyperparameter **BATCH_SIZE**): the reason is that, given the fact that we use a pretrained hugginface model and tokenizer, we cannot rely on our own vocabulary and the standard PyTorch classes (such as **Field**, **TranslationDataset**, and **Iterator**) to efficiently generate the tokenized batch in a transparent way.\n",
    "\n",
    "The tokenizer that we have to use relies on the codeBERT prebuilt vocabulary, and it tokenizes the whole dataset padding the inputs according to several criteria (for example the maximum length of the input in the dataset). This would mean that even input datapoints with short length would be padded to the length of the maximum one, generating a lot of overhead and heavily reducing the performance of the training and validation processes.\n",
    "\n",
    "The tokenizer let to truncate the long inputs, but given our input datapoints, truncating the input probably should not be an option. Indeed, by cutting off information, we may be removing important context or information that the model needs to make accurate predictions. This can result in decreased performance and reduced quality of the model's output.\n",
    "\n",
    "With the **DataProcessor** class we sort the data according to a given criteria (defined when the **DataProcessor** class is instantiated) and we already simulate the generation of batches so that we can tokenize batches of data instead of the whole dataset, reducing the padding to the longest input datapoint within each batch.\n",
    "\n",
    "The *temporary* simulated batches are then flattened to a sorted list of datapoints so that when they will be processed by the **DataLoader** sequentially, it will build the real tensor batches in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:08.868300Z",
     "start_time": "2023-05-15T15:46:08.814153Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    '''\n",
    "    The *DataProcessor* class takes care of pre-processing the raw datasets to create\n",
    "    the final datasets, efficiently sorted (with customizable criteria) and tokenized.\n",
    "    The datasets generated by the class can be feed to the *Dataset* and *DataLoader*\n",
    "    PyTorch classes to easily load the data and prepare the batches to feed the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenizer : RobertaTokenizerFast\n",
    "      The instance of tokenizer used to tokenize the input dataset\n",
    "    src : list[str]\n",
    "      The list of input datapoints (strings concatenated)\n",
    "    o_ids : list[int]\n",
    "      The list of oracle ids that identifies to which oracle each input datapoint refer\n",
    "    tgt : list[int]\n",
    "      The list of expected values (0 or 1) for each datapoint in input to the model\n",
    "    batch_size: int\n",
    "      The length of each batch of the training and validation dataset\n",
    "    training_ratio: float\n",
    "      A value in the interval [0.0,1.0] that represents the percentage of datapoints\n",
    "\n",
    "    Attributes\n",
    "    ------\n",
    "    tokenizer : RobertaTokenizerFast\n",
    "      The instance of tokenizer used to tokenize the input dataset\n",
    "    src : list[str]\n",
    "      The list of input datapoints (strings concatenated)\n",
    "    o_ids : list[int]\n",
    "      The list of oracle ids that identifies to which oracle each input datapoint refer\n",
    "    tgt : list[int]\n",
    "      The list of expected values (0 or 1) for each datapoint in input to the model\n",
    "    batch_size: int\n",
    "      The length of each batch of the training and validation dataset\n",
    "    training_ratio: float\n",
    "      A value in the interval [0.0,1.0] that represents the percentage of datapoints\n",
    "    processed_dataset: dict\n",
    "      Dictionary of the processed dataset:\n",
    "          - d_sorted: contains the tuples of (input, oracle_id, target) datapoints\n",
    "            after the original dataset has been sorted according to the selected criteria\n",
    "          - b_train_tokenized: contains the list of batches for the training dataset,\n",
    "            after the original dataset has been splitted (according to *training_ratio*\n",
    "            value), grouped in batches, and tokenized\n",
    "          - b_val_tokenized: contains the list of batches for the validation dataset,\n",
    "            after the original dataset has been splitted (according to *training_ratio*\n",
    "            value), grouped in batches, and tokenized\n",
    "          - b_train: contains the list of batches for the training dataset, after\n",
    "            the original dataset has been splitted (according to *training_ratio*\n",
    "            value) and grouped in batches, but not tokenized yet\n",
    "          - b_val: contains the list of batches for the training dataset, after\n",
    "            the original dataset has been splitted (according to *training_ratio*\n",
    "            value) and grouped in batches, but not tokenized yet\n",
    "          - b_short_id: given the number of datapoints in the original dataset,\n",
    "            the division in batches could be with rest. This means that one of the\n",
    "            batch will contains less datapoints than the others. Given that we\n",
    "            shuffle the batches during the dataprocessing, the class have to keep\n",
    "            track of the short batch so that it can be processed properly, during\n",
    "            the generation of the training and validation datasets\n",
    "          - rand_short_id: it is a boolean value that establish if in case of\n",
    "            a short batch, it must be kept as the last of the batches in the list,\n",
    "            (rand_short_id set to False) or it can have a random position within\n",
    "            the list (rand_short_id set to True)\n",
    "    '''\n",
    "    def __init__(self, tokenizer, src, o_ids, tgt, batch_size, training_ratio):\n",
    "        if not (len(src) == len(o_ids) and len(src) == len(tgt)):\n",
    "            raise Exception(\"[DataProcessor] the src, o_ids, and tgt lists must have the same length\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.src = src\n",
    "        self.o_ids = o_ids\n",
    "        self.tgt = tgt\n",
    "        self.batch_size = batch_size\n",
    "        self.training_ratio = training_ratio\n",
    "        self.processed_dataset = {\n",
    "            \"d_sorted\": [],\n",
    "            \"b_train_tokenized\": [],\n",
    "            \"b_val_tokenized\": [],\n",
    "            \"b_train\": [],\n",
    "            \"b_val\": [],\n",
    "            \"b_short_id\": -1,\n",
    "            \"rand_short_id\": True\n",
    "        }\n",
    "\n",
    "    def get_tokenized_dataset(self, d_type):\n",
    "        \"\"\"\n",
    "        The method Returns the final processed tokenized (training or validation) dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_type: DatasetType\n",
    "            The dataset type that the method must return\n",
    "                - DatasetType.TRAINING for the training dataset\n",
    "                - DatasetType.VALIDATION for the validation dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        t_dataset : TensorDataset\n",
    "            A PyTorch TensorDataset composed of three tensors stack:\n",
    "                - the first tensor stack representing the stack of tokenized input\n",
    "                  datapoints of the whole sorted dataset\n",
    "                - the second tensor stack representing the stack of attention masks\n",
    "                  (each index corresponding to the index of the input datapoints in\n",
    "                  the first tensor)\n",
    "                - the third tensor stack representing the list of expected target\n",
    "                  outputs (each index corresponding to the index of the input\n",
    "                  datapoints in the first tensor)\n",
    "        \"\"\"\n",
    "        # The list of batches of the tokenized (training or validation) datapoints\n",
    "        if d_type == DatasetType.TRAINING:\n",
    "            b_tokenized= self.processed_dataset[\"b_train_tokenized\"]\n",
    "        elif d_type == DatasetType.VALIDATION:\n",
    "            b_tokenized= self.processed_dataset[\"b_val_tokenized\"]\n",
    "        else:\n",
    "            raise Exception(f\"Unrecognized DataType value: {d_type}\")\n",
    "\n",
    "        t_dataset_list = []\n",
    "        # The batches are composed of tuples of (inputs, attention_masks, targets) s\n",
    "        # tacks, where each element of the tuple is a stack of n datapoints, with\n",
    "        # 1<=n<=*BATCH_SIZE*\n",
    "        #\n",
    "        #       t_batch = (\n",
    "        #           [\n",
    "        #               [t_i_1_1,...,t_i_1_n],\n",
    "        #                        ...\n",
    "        #               [t_i_k_1,...,t_i_k_n]\n",
    "        #           ],\n",
    "        #           [\n",
    "        #               [m_1_1,...,m_1_n],\n",
    "        #                        ...\n",
    "        #               [m_k_1,...,m_k_n]\n",
    "        #           ],\n",
    "        #           [\n",
    "        #               [t_1],\n",
    "        #                ...\n",
    "        #               [t_n]\n",
    "        #           ]\n",
    "        #       )\n",
    "        #\n",
    "        for t_batch in b_tokenized:\n",
    "            # The list of inputs of the current batch\n",
    "            t_src_batch = t_batch[0]\n",
    "            # The list of attention masks of the current batch\n",
    "            t_mask_batch = t_batch[1]\n",
    "            # The list of targets of the current batch\n",
    "            t_tgt_batch = t_batch[2]\n",
    "            # Generate a dataset of the batch\n",
    "            dataset_batch = TensorDataset(\n",
    "                t_src_batch,\n",
    "                t_mask_batch,\n",
    "                t_tgt_batch\n",
    "            )\n",
    "            # Add the datasets of batches\n",
    "            t_dataset_list.append(dataset_batch)\n",
    "        # Concatenates the datasets of batches in a single dataset\n",
    "        t_dataset = ConcatDataset(t_dataset_list)\n",
    "        # return the dataset\n",
    "        return t_dataset\n",
    "\n",
    "    def process_dataset(self, batch_type):\n",
    "        \"\"\"\n",
    "        This represents the core method of the class. Firstly, it sorts and maps the\n",
    "        original dataset into batches, according to the criterion passed as parameter\n",
    "        to the method itself (*batch_type*). Then, it splits the list of batches into\n",
    "        the training and validation datasets. Finally it tokenizes the batches of the\n",
    "        training and validation datasets\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_type: BatchType\n",
    "          The criterion type, according to which sort the input datapoints and generate\n",
    "          the batches for the training and the validation datasets\n",
    "        \"\"\"\n",
    "        # The dataset is sorted according to the batch type the PyTorch Dataloader will\n",
    "        # have to pruduce\n",
    "        if batch_type == BatchType.HETEROGENEOUS:\n",
    "          self._sort_dataset_heterogeneously()\n",
    "        elif batch_type == BatchType.OMOGENEOUS:\n",
    "          self._sort_dataset_omogeneously()\n",
    "        elif batch_type == BatchType.RANDOM:\n",
    "          self._sort_dataset_randomly()\n",
    "        elif batch_type == BatchType.SORTED_BY_LENGTH:\n",
    "          self._sort_dataset_by_input_length()\n",
    "        else:\n",
    "          raise Exception(\"Batch type not recognized.\")\n",
    "        # The sorted dataset is grouped in batches, and the batches are splitted in\n",
    "        # training and validation datasets\n",
    "        self._generate_train_val_batches()\n",
    "        # The batches of datapoints in the training and validation datasets are tokenized\n",
    "        self._tokenize_batches()\n",
    "\n",
    "    def _generate_train_val_batches(self):\n",
    "        \"\"\"\n",
    "        The method splits the datapoints of the sorted dataset, and generate the batches\n",
    "        for the training and the validation datasets.\n",
    "        \"\"\"\n",
    "        # The number of datapoints that compose the entire dataset\n",
    "        dp_len = len(self.processed_dataset[\"d_sorted\"])\n",
    "        # The number of datapoints that have to be assigned to the training dataset\n",
    "        dp_train_len = math.floor(dp_len * self.training_ratio)\n",
    "        # The number of datapoints that have to be assigned to the validation dataset\n",
    "        dp_val_len = dp_len - dp_train_len\n",
    "        # The total number of batches, given the total number of datapoints and the\n",
    "        # batch size. The division is rounded to the smallest integer greater than or\n",
    "        # equal to the result, because if there is a rest, the remaining datapoints\n",
    "        # have to be grouped in an additional batch\n",
    "        b_len = math.ceil(dp_len / self.batch_size)\n",
    "        # The total number of batches, given the number of datapoints assigned to the\n",
    "        # training dataset and the batch size.\n",
    "        b_train_len = math.ceil(dp_train_len / self.batch_size)\n",
    "        # The total number of batches, given the number of datapoints assigned to the\n",
    "        # validation dataset and the batch size\n",
    "        b_val_len = math.ceil(dp_val_len / self.batch_size)\n",
    "        # the whole dataset is grouped in batches\n",
    "        b_sorted_list = self._map_dataset_to_batches(b_len, dp_len)\n",
    "        # The batches of the whole dataset are shuffled\n",
    "        b_ids_shuffled_list = self._shuffle_sorted_batches_ids(b_len)\n",
    "\n",
    "        # The list of batches that compose the training dataset\n",
    "        b_train_list = []\n",
    "        # The list of batches that compose the validation dataset\n",
    "        b_val_list = []\n",
    "        # The for loop assigns the *training_ratio* percentage of the shuffled batches\n",
    "        # to the training dataset, while the remaining (1 - *training_ratio*) percentage\n",
    "        # to the validation dataset. The number of batches assigned to the training\n",
    "        # dataset is rounded up (for example if the number of batches is 22, and the\n",
    "        # training ratio is 0.8 the number of batces assigned to the training is\n",
    "        # 0.8 * 22 = 17.6 --> 18). The reason is that the number of batches assigned to\n",
    "        # the training an validation datasets must be an integer. The number of batches\n",
    "        # assigned to the validation dataset would be equal to (1 - 0.8) * 22 = 4.4 --> 5.\n",
    "        # In principle, this seems to lead to an inconsistent result because (given\n",
    "        # the example) the total number of batches would become:\n",
    "        #\n",
    "        #   b_train_len + b_val_len = 18 + 5 = 23 > 22\n",
    "        #\n",
    "        # But we can consider the fact that the \"extra\" batch is a batch shared between\n",
    "        # both the training and the validation datasets. The 80% of its content would\n",
    "        # be assigned to the training dataset, while the remaining 20% to the validation\n",
    "        # dataset.\n",
    "        # Moreover, the algorithm guarantees that the eventual batch splitted among the\n",
    "        # training and the validation datasets will always be a full batch, composed of\n",
    "        # BATCH_SIZE datapoints (if the whole dataset is composed of only one single\n",
    "        # batch, this batch is used instead, even if it is not full).\n",
    "        for idx, b_id in enumerate(b_ids_shuffled_list[:b_train_len],1):\n",
    "            if not idx == b_train_len:\n",
    "                b_train_list.append(b_sorted_list[b_id])\n",
    "            else:\n",
    "                # Check if the batch division has rest\n",
    "                if dp_train_len % self.batch_size == 0:\n",
    "                    # If the rest is 0 the whole batch is added to the training dataset\n",
    "                    b_train_list.append(b_sorted_list[b_id])\n",
    "                else:\n",
    "                    # If the rest is not 0 the batch is splitted among the training\n",
    "                    # and the validation datasets\n",
    "                    rest = dp_train_len % self.batch_size\n",
    "                    # The *training_ratio* percentage of the full batch is assigned to\n",
    "                    # the training dataset\n",
    "                    b_partial_train = b_sorted_list[b_id][:rest]\n",
    "                    # The remaining datapoints of the full batch is assigned to the\n",
    "                    # validation dataset\n",
    "                    b_partial_val = b_sorted_list[b_id][rest:]\n",
    "                    # Add the partially full batch to the training dataset batch list\n",
    "                    b_train_list.append(b_partial_train)\n",
    "                    # Add the remaining part of the batch to the validation dataset\n",
    "                    # batch list\n",
    "                    b_val_list.append(b_partial_val)\n",
    "        # The for loop assigns the remaining batches to the validation dataset\n",
    "        for b_id in b_ids_shuffled_list[b_train_len:]:\n",
    "            b_val_list.append(b_sorted_list[b_id])\n",
    "        # The generated lists of batches are stored in the *processed_dataset* dict\n",
    "        # of the instance of the class\n",
    "        self.processed_dataset[\"b_train\"] = b_train_list\n",
    "        self.processed_dataset[\"b_val\"] = b_val_list\n",
    "\n",
    "    def _map_dataset_to_batches(self, b_len, dp_len):\n",
    "        \"\"\"\n",
    "        The method maps the datapoints of the sorted dataset into batches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        b_len: int\n",
    "            The total number of batches within the whole dataset\n",
    "        dp_len: int\n",
    "            The total number of datapoints within the whole dataset\n",
    "        \"\"\"\n",
    "        # The list of the whole sorted dataset\n",
    "        d_sorted = self.processed_dataset[\"d_sorted\"]\n",
    "        # The rest of the division between the whole number of datapoints\n",
    "        # and the batch size. The rest gives the number of datapoints in\n",
    "        # the last batch (if the rest is 0 all the batches are full,\n",
    "        # otherwise, there is a batch that will contain *rest* datapoints,\n",
    "        # with rest < BATCH_SIZE\n",
    "        rest = dp_len % self.batch_size\n",
    "        # Boolean flag (True if there is a batch not full, False otherwise)\n",
    "        rest_flag = not rest == 0\n",
    "\n",
    "        # Check if there is a batch not full and the selected sorting\n",
    "        # algorithm let's to have the partially full batch in a random\n",
    "        # position\n",
    "        if rest_flag and self.processed_dataset[\"rand_short_id\"]:\n",
    "            # Select the random index of the partially full batch\n",
    "            short_batch_id = random.randrange(0,b_len)\n",
    "            # Store the index of the partially full batch\n",
    "            self.processed_dataset[\"b_short_id\"] = short_batch_id\n",
    "        else:\n",
    "            # If the position of the partially full batch cannot be\n",
    "            # random, get the position that it must have in the\n",
    "            # list of batches\n",
    "            short_batch_id = self.processed_dataset[\"b_short_id\"]\n",
    "        # Initialization of the batch list\n",
    "        batches = []\n",
    "        # Pointer of the position in the whole sorted dataset\n",
    "        pointer = 0\n",
    "        # The for cycle split the whole dataset into batches of size\n",
    "        # *BATCH_SIZE*. If the index of the current batch is equal\n",
    "        # to the index of the partially fulled batch (*short_batch_id*),\n",
    "        # a batch of size *rest* is generated\n",
    "        for i in range(b_len):\n",
    "          new_pointer = pointer\n",
    "          if rest_flag and i == short_batch_id:\n",
    "            # Compute the end position of the current batch in the whole\n",
    "            # dataset, if the current batch is the partially full batch\n",
    "            # of size *rest*)\n",
    "            new_pointer += rest\n",
    "          else:\n",
    "            # Compute the end position of the current batch in the whole\n",
    "            # dataset, if the current batch is a full batch of size\n",
    "            # *BATCH_SIZE*)\n",
    "            new_pointer += self.batch_size\n",
    "          # Extract batch datapoints from the whole sorted dataset\n",
    "          batch = d_sorted[pointer: new_pointer]\n",
    "          # Update pointer to the position of the whole sorted dataset\n",
    "          # not yet splitted\n",
    "          pointer = new_pointer\n",
    "          # Add the batch to the list\n",
    "          batches.append(batch)\n",
    "        # Return the batch list\n",
    "        return batches\n",
    "\n",
    "    def _shuffle_sorted_batches_ids(self, b_len,seed=42):\n",
    "        \"\"\"\n",
    "        The method shuffle the list of sorted batches. If there is a\n",
    "        partially full batch within the list, it is always positioned\n",
    "        at the end of the list (otherwise the following partition of\n",
    "        the batches between the training and the validation)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        b_len: int\n",
    "            The total number of batches in the whole dataset\n",
    "        seed: int\n",
    "            The seed through which the batch indices are randomly shuffled.\n",
    "            This parameter let to reproduce the same shuffled batches.\n",
    "        \"\"\"\n",
    "        # If the list has a partially full batch, get the index\n",
    "        # The value is -1 if there is not a short batch (all the\n",
    "        # batches in the list are full)\n",
    "        short_batch_id = self.processed_dataset[\"b_short_id\"]\n",
    "        # Generate a sorted list of indices\n",
    "        b_ids = [i for i in range(b_len)]\n",
    "        # Check if the partially full batch exists\n",
    "        if not short_batch_id == -1:\n",
    "            # Remove the index of the short batch\n",
    "            b_ids.remove(short_batch_id)\n",
    "        # Set the seed to reproduce the same shuffle in future\n",
    "        random.seed(seed)\n",
    "        random.shuffle(b_ids)\n",
    "        # Check if the partially full batch exists\n",
    "        if not short_batch_id == -1:\n",
    "            # Add the index of the short batch at the end of the list\n",
    "            b_ids.append(short_batch_id)\n",
    "        # Return the shuffled list of batch indices\n",
    "        return b_ids\n",
    "\n",
    "    def _sort_dataset_heterogeneously(self):\n",
    "        \"\"\"\n",
    "        The method sorts the original dataset distributing the datapoints heterogenously.\n",
    "        This sorting will let the dataloader to produce batches that minimize the\n",
    "        number of datapoints within each batch, referring to the same oracle (ideally\n",
    "        only 0 or 1 datapoint referring to the same oracle, for each batch)\n",
    "        \"\"\"\n",
    "        # Let's first create tuples of input, oracle ids and targets.\n",
    "        # This operation is necessary to maintain the conformity between the inputs,\n",
    "        # the oracle ids, and the targets that compose the dataset.\n",
    "        # Otherwise, if we only sort the inputs sentences, without sorting\n",
    "        # the corresponding oracle ids and targets, we lose the corrispondence between\n",
    "        # the input, the oracle id, and expected output.\n",
    "        # Therefore, given the list of the sentences, the list of the oracle ids, and\n",
    "        # the list of targets:\n",
    "        #\n",
    "        #     self.src = [s_1,s_2,...,s_n], where s_i is the input i within the dataset\n",
    "        #     self.o_ids = [o_1,o_2,...,o_n], where o_i is the oracle id i associated to the input i\n",
    "        #     self.tgt = [t_1,t_2,...,t_n], where t_i is the label associated to the input i\n",
    "        #\n",
    "        # the zip statement produces a list of (s_i, o_i, t_i) tuples:\n",
    "        #\n",
    "        #     src_tgt_zip = [(s_1,o_1,t_1),(s_2,o_1,t_2),...,(s_n,o_1,t_n)]\n",
    "        #\n",
    "        s_o_t_zip = list(zip(self.src, self.o_ids, self.tgt))\n",
    "\n",
    "        # Length of the whole dataset\n",
    "        dp_len = len(self.src)\n",
    "        # Number of total batches, given the whole dataset\n",
    "        b_len = math.ceil(dp_len / self.batch_size)\n",
    "        # Dictionary of the number of occurrences of each oracle id within the whole dataset.\n",
    "        # The Counter class, given a list, computes the number of occurrences for each different\n",
    "        # value in the list. For example, given the list\n",
    "        #\n",
    "        #       o_ids = [1,2,2,4,3,2,1,3,2,4]\n",
    "        #\n",
    "        # The Counter class returns the following dictionary:\n",
    "        #\n",
    "        #       occurrences_counter = {\n",
    "        #           1: 2,\n",
    "        #           2: 4,\n",
    "        #           3: 2,\n",
    "        #           4: 2\n",
    "        #       }\n",
    "        #\n",
    "        occurrences_counter = Counter(self.o_ids)\n",
    "        # Dictionary of empty lists, one for each oracle id in the dataset. each list, will be\n",
    "        # filled with the datapoints that refer to the same oracle id. Given the previous\n",
    "        # example of *o_ids* and *occurrences_counter*, the *occurrence* dictionary will be\n",
    "        # initialized in this way:\n",
    "        #\n",
    "        #       occurrences = {\n",
    "        #           1: [],\n",
    "        #           2: [],\n",
    "        #           3: [],\n",
    "        #           4: []\n",
    "        #       }\n",
    "        #\n",
    "        occurrences = { k: [] for k in occurrences_counter.keys()}\n",
    "        # The list of the final sorted tuples (s_i,o_i,t_i)\n",
    "        s_o_t_zip_sorted = []\n",
    "\n",
    "        # The for cycle fills the occurrences of tuples of datapoints in the dataset that\n",
    "        # refers to the same oracle id\n",
    "        #\n",
    "        #       occurrences = {\n",
    "        #           1: [(s_1_1,o_1_1,t_1_1),(s_1_2,o_1_2,t_1_2)]\n",
    "        #           2: [(s_2_1,o_2_1,t_2_1),...,(s_2_4,o_2_4,t_2_4)],\n",
    "        #           3: [(s_3_1,o_3_1,t_3_1),(s_3_2,o_3_2,t_3_2)],\n",
    "        #           4: [(s_4_1,o_4_1,t_4_1),(s_4_2,o_4_2,t_4_2)]\n",
    "        #       }\n",
    "        #\n",
    "        for input, oracle_id, target in s_o_t_zip:\n",
    "            occurrences[oracle_id].append((input,oracle_id,target))\n",
    "\n",
    "        # The for cycle sorts the dataset simulating the filling of the batches of the\n",
    "        # whole dataset, trying to create heterogeneous batches\n",
    "        for i in range(b_len):\n",
    "            #  The list of distinct oracle ids still available\n",
    "            o_avail = list(occurrences_counter.keys())\n",
    "            # Fills the batch of *BATCH_SIZE* length\n",
    "            for j in range(self.batch_size):\n",
    "                #  Check id the list of distinct oracle ids still available is empty\n",
    "                if len(o_rand) == 0:\n",
    "                    # If the list is empty try to refill it with the distinct oracle ids\n",
    "                    # still available in the counter dictionary\n",
    "                    o_avail = list(occurrences_counter.keys())\n",
    "                    # Check if the list is empty even after the re-filling\n",
    "                    if o_avail == 0:\n",
    "                        # If the list is empty stops the algorithm\n",
    "                        break\n",
    "                o_id_rand = random.choice(o_avail)\n",
    "                o_rand = random.choice(occurrences[o_id_rand])\n",
    "                o_avail.remove(o_id_rand)\n",
    "                occurrences[o_id_rand].remove(o_rand)\n",
    "                occurrences_counter[o_rand] -= 1\n",
    "                if occurrences_counter[o_rand] == 0:\n",
    "                  assert len(occurrences[o_id_rand]) == 0, \"Occurences should be empty.\"\n",
    "                  del occurrences_counter[o_id_rand]\n",
    "                s_o_t_zip_sorted.append(o_rand)\n",
    "        self.processed_dataset[\"d_sorted\"] = s_o_t_zip_sorted\n",
    "        self.processed_dataset[\"b_short_id\"]: b_len - 1\n",
    "        self.processed_dataset[\"rand_short_id\"]: False\n",
    "\n",
    "    def _sort_dataset_omogeneously(self):\n",
    "        \"\"\"\n",
    "        The method sorts the dataset in a way that let the dataloader to produce\n",
    "        batches with the highest number of datapoints referring to the same oracle\n",
    "        \"\"\"\n",
    "        # Let's first create tuples of input, oracle ids and targets.\n",
    "        # This operation is necessary to maintain the conformity between the inputs,\n",
    "        # the oracle ids, and the targets that compose the dataset.\n",
    "        # Otherwise, if we only sort the inputs sentences, without sorting\n",
    "        # the corresponding oracle ids and targets, we lose the corrispondence between\n",
    "        # the input, the oracle id, and expected output.\n",
    "        # Therefore, given the list of the sentences, the list of the oracle ids, and\n",
    "        # the list of targets:\n",
    "        #\n",
    "        #     self.src = [s_1,s_2,...,s_n], where s_i is the input i within the dataset\n",
    "        #     self.o_ids = [o_1,o_2,...,o_n], where o_i is the oracle id i associated to the input i\n",
    "        #     self.tgt = [t_1,t_2,...,t_n], where t_i is the label associated to the input i\n",
    "        #\n",
    "        # the zip statement produces a list of (s_i, o_i, t_i) tuples:\n",
    "        #\n",
    "        #     src_tgt_zip = [(s_1,o_1,t_1),(s_2,o_1,t_2),...,(s_n,o_1,t_n)]\n",
    "        #\n",
    "        s_o_t_zip = list(zip(self.src, self.o_ids, self.tgt))\n",
    "        # The tuples are sorted using as key the second element of each tuple, which means\n",
    "        # the oracle id of each input.\n",
    "        s_o_t_zip_sorted = sorted(s_o_t_zip, key=lambda t: t[1])\n",
    "        self.processed_dataset[\"d_sorted\"] = s_o_t_zip_sorted\n",
    "\n",
    "    def _sort_dataset_randomly(self, seed=42):\n",
    "        \"\"\"\n",
    "        The method shuffle the original dataset in a random way\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        seed: int\n",
    "          The seed through which the datapoints of the dataset are randomly shuffled.\n",
    "          This parameter let to reproduce the same shuffled batches.\n",
    "        \"\"\"\n",
    "        # The batches are randomly sampled from the dataset\n",
    "        s_o_t_zip = list(zip(self.src, self.o_ids, self.tgt))\n",
    "        # Set the seed to reproduce the same shuffle in future\n",
    "        random.seed(seed)\n",
    "        # Shuffle the datasets tuples randomly\n",
    "        random.shuffle(s_o_t_zip)\n",
    "        self.processed_dataset[\"d_sorted\"] = s_o_t_zip\n",
    "\n",
    "    def _sort_dataset_by_input_length(self):\n",
    "        \"\"\"\n",
    "        The method sorts the dataset in a way that let the dataloader to produce\n",
    "        batches of data composed of inputs with similar length. In this way, when\n",
    "        the class will tokenize and embed the batches, it will reduce the padding in each\n",
    "        batch, improving the performance of the model training (padding is used to\n",
    "        create sentences with the same length, but they do not contribute to the\n",
    "        training of the model and they introduce overhead)\n",
    "        \"\"\"\n",
    "        # The method sort the dataset in a way that let the dataloader to produce\n",
    "        # batches of data composed of inputs with similar length. In this way, when\n",
    "        # we will tokenize and embed the batches, we will reduce the padding in each\n",
    "        # batch, improving the performance of the model training (padding is used to\n",
    "        # create sentences with the same length, but they do not contribute to the\n",
    "        # training of the model and they introduce overhead).\n",
    "        #\n",
    "        # Let's first create tuples of input, oracle ids and targets.\n",
    "        # This operation is necessary to maintain the conformity between the inputs,\n",
    "        # the oracle ids, and the targets that compose the dataset.\n",
    "        # Otherwise, if we only sort the inputs sentences, without sorting\n",
    "        # the corresponding oracle ids and targets, we lose the corrispondence between\n",
    "        # the input, the oracle id, and expected output.\n",
    "        # Therefore, given the list of the sentences, the list of the oracle ids, and\n",
    "        # the list of targets:\n",
    "        #\n",
    "        #     self.src = [s_1,s_2,...,s_n], where s_i is the input i within the dataset\n",
    "        #     self.o_ids = [o_1,o_2,...,o_n], where o_i is the oracle id i associated to the input i\n",
    "        #     self.tgt = [t_1,t_2,...,t_n], where t_i is the label associated to the input i\n",
    "        #\n",
    "        # the zip statement produces a list of (s_i, o_i, t_i) tuples:\n",
    "        #\n",
    "        #     src_tgt_zip = [(s_1,o_1,t_1),(s_2,o_1,t_2),...,(s_n,o_1,t_n)]\n",
    "        #\n",
    "        s_o_t_zip = list(zip(self.src, self.o_ids, self.tgt))\n",
    "        # The tuples are sorted using as key the first element of each tuple, which means\n",
    "        # the oracle id of each input.\n",
    "        s_o_t_zip_sorted = sorted(s_o_t_zip, key=lambda t: t[0])\n",
    "        self.processed_dataset[\"d_sorted\"] = s_o_t_zip_sorted\n",
    "\n",
    "    def _tokenize_batches(self):\n",
    "        \"\"\"\n",
    "        The method tokenizes the input datapoints of the batches that composes the\n",
    "        training and validation datasets\n",
    "        \"\"\"\n",
    "        # The batch list of the training dataset\n",
    "        b_train = self.processed_dataset[\"b_train\"]\n",
    "        # The batch list of the validation dataset\n",
    "        b_val = self.processed_dataset[\"b_val\"]\n",
    "        for d_type in DatasetType:\n",
    "            if d_type == DatasetType.TRAINING:\n",
    "                batches = b_train\n",
    "            elif d_type == DatasetType.VALIDATION:\n",
    "                batches = b_val\n",
    "            else:\n",
    "                raise Exception(f\"Unrecognized DataType value: {d_type}\")\n",
    "            for batch in batches:\n",
    "                # Extracts the inputs datapoints from the batch\n",
    "                b_inputs = [ t[0] for t in batch ]\n",
    "                # Extracts the corresponding targets datapoints from the batch\n",
    "                b_targets = [ t[2] for t in batch ]\n",
    "                # Computes the length of the longest input datapoint within the batch\n",
    "                max_len = reduce(lambda max_len, s: len(s) if len(s) > max_len else max_len, b_inputs,0)\n",
    "                # Tokenize the inputs datapoints of the batch\n",
    "                # The method generate a dictionary with two keys:\n",
    "                #\n",
    "                #   t_src_dict = {\n",
    "                #       \"input_ids\": [[t_i_1_1,...,t_i_1_n],...,[t_i_k_1,...,t_i_k_n]],\n",
    "                #       \"attention_mask\": [[m_1_1,...,m_k_n],...,[m_k_1,...,m_k_n]]\n",
    "                #   }\n",
    "                #\n",
    "                # where each element in the *input_ids* list is a list of tokenized words\n",
    "                # (the words of an input datapoint), while each element in the *attention\n",
    "                # masks* is the corresponding mask vector to distinguish the real tokens\n",
    "                # from the padding tokens. In the example, t_i_x_y is the y tokenized word\n",
    "                # of the input datapoint x, and m_x_y is a boolean value that states if\n",
    "                # the token y is a real word or a padding token\n",
    "                #\n",
    "                t_src_dict = self.tokenizer(\n",
    "                  b_inputs,\n",
    "                  max_length=max_len,\n",
    "                  padding='max_length',\n",
    "                  truncation=True\n",
    "                )\n",
    "                # Transform the list into a tensor stack\n",
    "                #\n",
    "                #   t_src_dict['input_ids'] = [[t_i_1_1,...,t_i_1_n],...,[t_i_k_1,...,t_i_k_n]]\n",
    "                #\n",
    "                #   t_inputs = tensor([\n",
    "                #           [t_i_1_1,...,t_i_1_n],\n",
    "                #                   ...\n",
    "                #           [t_i_k_1,...,t_i_k_n]\n",
    "                #   ])\n",
    "                #\n",
    "                # this is the structure accepted by the DataLoader, to process the dataset\n",
    "                t_inputs = torch.stack([torch.tensor(ids) for ids in t_src_dict['input_ids']])\n",
    "                # Transform the list into a tensor stack\n",
    "                t_attention_masks = torch.stack([torch.tensor(mask) for mask in t_src_dict['attention_mask']])\n",
    "                # Transform the targets into a tensor list\n",
    "                targets_tensor = torch.tensor(b_targets)\n",
    "                if d_type == DatasetType.TRAINING:\n",
    "                    # Add the tuple representing the tokenized batch to the list of training dataset\n",
    "                    self.processed_dataset[\"b_train_tokenized\"].append((t_inputs, t_attention_masks, targets_tensor))\n",
    "                elif d_type == DatasetType.VALIDATION:\n",
    "                    # Add the tuple representing the tokenized batch to the list of validation dataset\n",
    "                    self.processed_dataset[\"b_val_tokenized\"].append((t_inputs, t_attention_masks, targets_tensor))\n",
    "                else:\n",
    "                    raise Exception(f\"Unrecognized DataType value: {d_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:18.103065Z",
     "start_time": "2023-05-15T15:46:09.482770Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create DataProcessor instance\n",
    "data_processor = DataProcessor(tokenizer,src_list,oracle_ids_list,tgt_list,BATCH_SIZE,TRAINING_RATIO)\n",
    "# Process the data\n",
    "data_processor.process_dataset(BatchType.RANDOM)\n",
    "# Get the train and validation sorted datasets\n",
    "train_dataset = data_processor.get_tokenized_dataset(DatasetType.TRAINING)\n",
    "val_dataset = data_processor.get_tokenized_dataset(DatasetType.VALIDATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataloader\n",
    "\n",
    "DataLoader is a pytorch class that takes care of shuffling/sampling/weigthed\n",
    "sampling, batching, and using multiprocessing to load the data, in an efficient\n",
    "and transparent way.\n",
    "We define a dataloader for both the training and the validation dataset.\n",
    "\n",
    "The dataloader generates the real batches of datapoints that we will use to\n",
    "feed the model.\n",
    "\n",
    "We use an helper PyTorch class, **SequentialSampler**, to create the batches\n",
    "selecting the datapoints sequentially, from the training and validation datasets.\n",
    "Indeed, we used the **DataProcessor** class to sort the dataset in specific way,\n",
    "simulating the creation of batches of data before the **DataLoader**, minimizing\n",
    "the padding (in the case of *BatchType.HOMOGENEOUS*) or maximizing the\n",
    "diversity within the dataset (in the case of *BatchType.HETEROGENEOUS*). The\n",
    "use of the **SequentialSampler** will guarantee to maintain this criteria for\n",
    "the creation of the batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:18.109635Z",
     "start_time": "2023-05-15T15:46:18.105991Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creation of the training and validation dataloaders\n",
    "dl_train = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=SequentialSampler(train_dataset),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "dl_val = DataLoader(val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model\n",
    "The **OracleClassifier** represents our fine-tuned model.\n",
    "\n",
    "The architecture of the model is composed of:\n",
    "\n",
    "1. The pre-trained codeBERT Transformer model\n",
    "2. A fully-connected layer that takes in input the output from the codebert model (which represents our hidden state) and maps this vector to a a vector of two elements (representing our 0 and 1 scores)\n",
    "3. The softmax activation function that transforms the output vector of the fully-connected layer into a vector of n probabilities, given the n classes of the classification task (in our case, 2). The softmax activation function is computed implicitly, during the training phase, by the loss function (the PyTorch **CrossEntropy** class, in our case). Therefore, the softmax is not a visible layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:22.763074Z",
     "start_time": "2023-05-15T15:46:22.754559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OracleClassifier(Module):\n",
    "    \"\"\"\n",
    "    The model is composed of:\n",
    "        1.  The pretrained codeBERT model\n",
    "        2.  A fully connected layer that takes in input the output from the\n",
    "            codebert model (which represents our hidden state) and maps this vector\n",
    "            to a a vector of two elements (representing our 0 and 1 scores).\n",
    "    N.B.: The vector in output to our model is not normalized, therefore each\n",
    "          element does not represent the probability that the token is 0 or 1\n",
    "          by itself. The normalization is performed by the cross entropy loss\n",
    "          function, which performs the softmax.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_input_len: int\n",
    "        The length of the longest input in the whole dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, max_input_len=512, device=None):\n",
    "        super(OracleClassifier, self).__init__()\n",
    "        # First layer of the model: the pre-trained codeBERT model\n",
    "        self.codebert_transformer = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        # Setup of the pre-trained model layer\n",
    "        self.codebert_transformer.config.max_position_embeddings = max_input_len\n",
    "        self.codebert_transformer.base_model.embeddings.position_ids = torch.arange(max_input_len).expand((1, -1))\n",
    "        self.codebert_transformer.base_model.embeddings.token_type_ids = torch.zeros(max_input_len).expand((1, -1)).int()\n",
    "        orig_pos_emb = self.codebert_transformer.base_model.embeddings.position_embeddings.weight\n",
    "        self.codebert_transformer.base_model.embeddings.position_embeddings.weight = torch.nn.Parameter(torch.cat((orig_pos_emb, orig_pos_emb)))\n",
    "\n",
    "        # The last layer of the pre-trained codeBERT model. It is necessary to\n",
    "        # get the size of the output layer and understand the size of the next\n",
    "        # fully-connected layer (the output vector of the pre-trained codeBERT\n",
    "        # model will represent the input vector of the fully-connected layer)\n",
    "        hidden_size = self.codebert_transformer.config.to_dict()['hidden_size']\n",
    "        # Second layer of the model: the fully-connected layer.\n",
    "        # The size of the input is equal to the dimension of the output vector\n",
    "        # of the pre-trained codeBERT model, while the size of the output is\n",
    "        # a vector of two elements (the two classes of our classifier)\n",
    "        self.linear = Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, input_masks):\n",
    "        \"\"\"\n",
    "        The method feed the model with the stack of inputs and the attention masks.\n",
    "        It returns the stack of output vectors of the last fully-connected layer.\n",
    "        Each output vector of the stack represents the non-normalized vector of\n",
    "        the probabilities that the corresponding input belongs to each class of\n",
    "        the classification task.\n",
    "\n",
    "        For example:\n",
    "\n",
    "            input_ids = tensor([\n",
    "                            [t_i_1_1,...,t_i_1_n],\n",
    "                                     ...\n",
    "                            [t_i_k_1,...,t_i_k_n]\n",
    "                        ])\n",
    "\n",
    "        where each row of the stack is a tokenized input (t_i_1_1 is the first\n",
    "        token of the first word of the first input of the batch).\n",
    "\n",
    "            input_masks = tensor([\n",
    "                            [m_1_1,...,m_1_n],\n",
    "                                     ...\n",
    "                            [m_k_1,...,m_k_n]\n",
    "                          ])\n",
    "\n",
    "        where each row of the stack is the corresponding attention mask of the\n",
    "        input with the same index in the input_ids stack.\n",
    "\n",
    "        The output is a stack of the form:\n",
    "\n",
    "            output = tensor([\n",
    "                        [p_1,p_2],\n",
    "                           ...\n",
    "                        [p_1,p_2]\n",
    "                     ])\n",
    "\n",
    "        where each row of the stack is the corresponding output of the input with\n",
    "        the same index in the input_ids stack. The row is a vector composed of two\n",
    "        elements (in our case) and p_1 and p_2 represents the non-normalized\n",
    "        probabilities that the input belongs respectively to the first and the\n",
    "        second class of the classificator (the softmax computed in the loss function,\n",
    "        during the training phase transforms p_1 and p_2 into normalized values,\n",
    "        i.e. real probability values whose sum is 1).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids: Tensor\n",
    "            The tensor stack of the inputs within the batch passed to the model\n",
    "            in the training or validation phase\n",
    "        input_masks: Tensor\n",
    "            The tensor stack of the attention masks within the batch passed to\n",
    "            the model in the training or validation phase\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output: Tensor\n",
    "            The tensor stack of the outputs of the model\n",
    "        \"\"\"\n",
    "        output = self.codebert_transformer(input_ids, input_masks)\n",
    "        output = output.pooler_output\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:33.871526Z",
     "start_time": "2023-05-15T15:46:30.771770Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# We compute the maximum length of the input datapoints, within the whole dataset\n",
    "# This let us to guarantee that the model will process input data of this length\n",
    "# The +2 is given by the fact that the model add the start token and the end token\n",
    "# to each input of the model.\n",
    "max_input_len = reduce(lambda max_len, s: len(s) if len(s) > max_len else max_len, src_list,0) + 2\n",
    "# Creation of the instance of the model\n",
    "model = OracleClassifier(max_input_len)\n",
    "# The model is loaded on the gpu (or cpu, if not available)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training\n",
    "\n",
    "The **OracleTrainer** class is an helper class that is used to perform the training\n",
    "and the validation phases of the model. During the training phase, the model uses the\n",
    "batches of data to compute the loss and update the weights to improve the accuracy of\n",
    "the predictions. Instead, in the validation phase the trainer use batches of the\n",
    "validation dataset to evaluate how the model is able to generalize on unseen data.\n",
    "During the validation phase the weights of the model are not updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:34.957594Z",
     "start_time": "2023-05-15T15:46:34.922574Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OracleTrainer():\n",
    "    \"\"\"\n",
    "    The *OracleTrainer* class is an helper class that, given the loss\n",
    "    function, the optimizer, the model, and the training and validation\n",
    "    datasets perform the training of the model and computes the loss and\n",
    "    the accuracy of the training and validation phases, saves the statistics\n",
    "    of the training, and have auxiliary methods that let to visualize the\n",
    "    trend of the training and the validation, over the epochs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: OracleClassifier\n",
    "            The model to train\n",
    "    loss_fn:\n",
    "        The loss function to compute the loss, during the training and\n",
    "        validation phases\n",
    "    optimizer:\n",
    "        The optimizer used to perform the backpropagation and updates\n",
    "        the weights of the model\n",
    "    dl_train: DataLoader\n",
    "        The training dataloader which contains the batches of datapoints\n",
    "        for the training phase\n",
    "    dl_val: DataLoader\n",
    "        The validation dataloader which contains the batches of datapoints\n",
    "        for the validation phase\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer, dl_train, dl_val):\n",
    "        self.model = model\n",
    "        self.dl_train = dl_train\n",
    "        self.dl_val = dl_val\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, num_epochs, break_end = 99.9, best_time = math.inf):\n",
    "        \"\"\"\n",
    "        The method perform the training and validation phases of the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_epochs: int\n",
    "            The number of epochs to train the model\n",
    "        break_end:\n",
    "            The accuracy threshold to stop the training\n",
    "        best_time:\n",
    "            Best time for statistics performance\n",
    "        \"\"\"\n",
    "        print(\"Start Training...\")\n",
    "\n",
    "        # Dictionary of the statistics\n",
    "        stats = {\n",
    "            't_loss': [],\n",
    "            'v_loss': [],\n",
    "            't_accuracy': [],\n",
    "            'v_accuracy': []\n",
    "        }\n",
    "        steps = 0\n",
    "        accumulation_steps = 8\n",
    "        flag_90 = False\n",
    "        flag_end = False\n",
    "        time_over = False\n",
    "\n",
    "        # In each epoch the trainer train the model batch by batch,\n",
    "        # with all the batch of the training dataset. After a given\n",
    "        # number of *accumulation_steps* the trainer performs the\n",
    "        # backpropagation and updates the weights accordingly.\n",
    "        # Moreover, it computes the accuracy and the total loss of\n",
    "        # the training, and performs the validation to understand how\n",
    "        # well the model generalize on the validation data.\n",
    "        for epoch in range(1, num_epochs +1):\n",
    "            total_loss = 0\n",
    "            total_accuracy = 0\n",
    "            trained_total = 0\n",
    "            predicted_correct = 0\n",
    "\n",
    "            start = timeit.default_timer()\n",
    "\n",
    "            # model in training mode\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            for step, batch in enumerate(self.dl_train):\n",
    "                print(f\"processing step {step+1} of {len(self.dl_train)}\")\n",
    "                steps += 1\n",
    "\n",
    "                # Extract the inputs, the attention masks and the expected\n",
    "                # outputs from the batch\n",
    "                src_input = batch[0].to(DEVICE)\n",
    "                masks_input = batch[1].to(DEVICE)\n",
    "                tgt_out = batch[2].to(DEVICE)\n",
    "\n",
    "                # Train the model\n",
    "                outputs = self.model(src_input, masks_input)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = self.loss_fn(outputs, tgt_out)\n",
    "                loss.backward()\n",
    "\n",
    "                # Exctract the predicted values and the expected output\n",
    "                with torch.no_grad():\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    _, expected_out = tgt_out.max(1)\n",
    "                # Update the accuracy of the model, given the predictions\n",
    "                # of the batch\n",
    "                trained_total += tgt_out.size(0)\n",
    "                predicted_correct += (predicted == expected_out).sum().item()\n",
    "\n",
    "                if (steps % accumulation_steps) == 0:\n",
    "                    # Update the weights of the model\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    # Update the total loss\n",
    "                    total_loss += loss.item()\n",
    "                    # Compute the accuracy of the model within the accumulation\n",
    "                    # steps\n",
    "                    total_accuracy = 100 * predicted_correct/trained_total\n",
    "                    # Reset the counter for the accuracy\n",
    "                    trained_total = 0\n",
    "                    predicted_correct = 0\n",
    "\n",
    "                if (steps % N_STEPS) == 0:\n",
    "                    # Compute average statistics for the loss and the accuracy\n",
    "                    mean_t_loss = total_loss / (N_STEPS / accumulation_steps)\n",
    "                    mean_t_accuracy = total_accuracy / (N_STEPS / accumulation_steps)\n",
    "\n",
    "                    # Validation phase\n",
    "                    mean_v_loss, mean_v_accuracy = self.validation()\n",
    "\n",
    "                    # Update the statistics\n",
    "                    stats['t_loss'].append(mean_t_loss)\n",
    "                    stats['t_accuracy'].append(mean_t_accuracy)\n",
    "                    stats['v_loss'].append(mean_v_loss)\n",
    "                    stats['v_accuracy'].append(mean_v_accuracy)\n",
    "\n",
    "                    # Print the statistics\n",
    "                    self.print_stats(\n",
    "                        epoch,\n",
    "                        num_epochs,\n",
    "                        step + 1,\n",
    "                        len(self.dl_train),\n",
    "                        mean_t_loss,\n",
    "                        mean_t_accuracy,\n",
    "                        mean_v_loss,\n",
    "                        mean_v_accuracy\n",
    "                    )\n",
    "\n",
    "                    # Reset the total loss and accuracy\n",
    "                    total_loss = 0\n",
    "                    total_accuracy = 0\n",
    "                    interval = timeit.default_timer()\n",
    "\n",
    "                    # Breakpoint validation accuracy\n",
    "                    if mean_v_accuracy > 90 and (not flag_90):\n",
    "                      flag_90 = not flag_90\n",
    "                      print('-'*30)\n",
    "                      print(\"BREAKPOINT 90% VALIDATION ACCURACY\")\n",
    "                      print('-'*30)\n",
    "                      print(f\"TIME: {int(interval - start)}seconds\")\n",
    "                      print('-'*30)\n",
    "                      stats['time_90'] = int(interval - start)\n",
    "\n",
    "                    # Breakpoint validation accuracy - stop the\n",
    "                    # training to avoid overfitting\n",
    "                    if mean_v_accuracy > break_end:\n",
    "                      flag_end = not flag_end\n",
    "                      interval = timeit.default_timer()\n",
    "                      print('-'*30)\n",
    "                      print(\"BREAKPOINT 100% VALIDATION ACCURACY\")\n",
    "                      print('-'*30)\n",
    "                      print(f\"TIME: {int(interval - start)} seconds\")\n",
    "                      print('-'*30)\n",
    "                      print('-'*30)\n",
    "                      print(f\"FINAL SAMPLES\")\n",
    "                      print('-'*30)\n",
    "                      stats['time_100'] = int(interval - start)\n",
    "                      break\n",
    "\n",
    "                    # If time is over, stop the training\n",
    "                    if int(interval - start) > best_time or int(interval - start) > 6000:\n",
    "                      time_over = True\n",
    "                      stats['time_100'] = math.inf\n",
    "                      break\n",
    "            if flag_end or time_over:\n",
    "                break\n",
    "        return stats\n",
    "\n",
    "    def print_stats(self,epoch,num_epochs, step, total_steps, mean_t_loss, mean_t_accuracy, mean_v_loss, mean_v_accuracy):\n",
    "        \"\"\"\n",
    "        The method prints the statistics of the training and validation phases\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epoch: int\n",
    "            The current epoch of the training\n",
    "        num_epochs: int\n",
    "            The total number of epochs\n",
    "        step: int\n",
    "            The current step of the training phase, in the current epoch\n",
    "        total_steps: int\n",
    "            The total number of steps within an epoch\n",
    "        mean_t_loss: float\n",
    "            Average training loss\n",
    "        mean_t_accuracy: float\n",
    "            Average training accuracy\n",
    "        mean_v_loss: float\n",
    "            Average validation loss\n",
    "        mean_v_accuracy:\n",
    "            Average validation accuracy\n",
    "        \"\"\"\n",
    "        print('-'*30)\n",
    "        print(\"STATISTICS\")\n",
    "        print('-'*30)\n",
    "        print(f\"EPOCH: [{epoch} / {num_epochs}]\")\n",
    "        print(f\"STEP: [{step} / {total_steps}]\")\n",
    "        print(f\"TRAINING LOSS: {(mean_t_loss):.4f}\")\n",
    "        print(f\"TRAINING ACCURACY: {(mean_t_accuracy):.2f}%\")\n",
    "        print('-'*30)\n",
    "        print(f\"VALIDATION LOSS: {(mean_v_loss):.4f}\")\n",
    "        print(f\"VALIDATION ACCURACY: {(mean_v_accuracy):.2f}%\")\n",
    "        print('-'*30)\n",
    "\n",
    "    def plot_loss_accuracy(self,steps, ax, stats):\n",
    "        \"\"\"\n",
    "        The method plots the trend of the loss and the accuracy over the epochs\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps: int\n",
    "            The number of steps\n",
    "        ax:\n",
    "            The axes of the matplotlib figure\n",
    "        stats: dict\n",
    "            The dictionary of the statistics\n",
    "        \"\"\"\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                title = ('Training ' if j == 0 else 'Validation') + ('Loss' if i == 0 else 'Accuracy')\n",
    "                dict_label = ('t_' if j == 0 else 'v_') + ('loss' if i == 0 else 'accuracy')\n",
    "                color = 'blue'\n",
    "                ax[i][j].set_title(title, fontsize=30)\n",
    "                ax[i][j].set_xlabel(\"steps\", fontsize=30)\n",
    "                ax[i][j].set_ylabel(\"loss\", fontsize=30)\n",
    "                ax[i][j].plot(range(0,len(stats['t_loss'])*N_STEPS,N_STEPS), np.array(stats[dict_label])[:], '-', color=color)\n",
    "        for i in range(2):\n",
    "            title = 'Training and Validation ' + ('Loss' if i == 0 else 'Accuracy')\n",
    "            dict_label = 'loss' if i == 0 else 'accuracy'\n",
    "            ax[2][i].set_title(title, fontsize=30)\n",
    "            ax[2][i].set_xlabel(\"steps\", fontsize=30)\n",
    "            ax[2][i].set_ylabel(\"loss\", fontsize=30)\n",
    "            ax[2][i].plot(range(0,len(stats['t_loss'])*N_STEPS,N_STEPS), np.array(stats['t_' + dict_label])[:], '-', color='blue')\n",
    "            ax[2][i].plot(range(0,len(stats['t_loss'])*N_STEPS,N_STEPS), np.array(stats['v_' + dict_label])[:], '-', color='orange')\n",
    "\n",
    "    def validation(self):\n",
    "        \"\"\"\n",
    "        The method computes the validation phase.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mean_v_loss: float\n",
    "            Average loss of the validation phase\n",
    "        mean_v_accuracy:\n",
    "            Average accuracy of the validation phase\n",
    "        \"\"\"\n",
    "        # model in evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        trained_total = 0\n",
    "        predicted_total = 0\n",
    "        total_steps = 0\n",
    "        # The validation phase is performed without accumulating\n",
    "        # the gradient descent and without updating the weights\n",
    "        # of the model\n",
    "        with torch.no_grad():\n",
    "            for batch in iter(self.dl_val):\n",
    "                total_steps += 1\n",
    "                # Extract the inputs, the attention masks and the\n",
    "                # targets from the batch\n",
    "                src_input = batch[0].to(DEVICE)\n",
    "                masks_input = batch[1].to(DEVICE)\n",
    "                tgt_out = batch[2].to(DEVICE)\n",
    "                # Feed the model\n",
    "                outputs = self.model(src_input, masks_input)\n",
    "                # Compute the loss\n",
    "                loss = self.loss_fn(outputs, tgt_out)\n",
    "                total_loss += loss.item()\n",
    "                # Exctract the predicted values and the expected output\n",
    "                with torch.no_grad():\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    _, expected_out = tgt_out.max(1)\n",
    "                # Update the accuracy of the model, given the predictions\n",
    "                # of the batch\n",
    "                trained_total += tgt_out.size(0)\n",
    "                predicted_total += (predicted == expected_out).sum().item()\n",
    "                # Update the accuracy\n",
    "                total_accuracy += 100 * predicted_total/trained_total\n",
    "        # Compute the average validation loss\n",
    "        mean_v_loss = total_loss / len(self.dl_val)\n",
    "        # Compute the average validation accuracy\n",
    "        mean_v_accuracy = total_accuracy / len(self.dl_val)\n",
    "        return mean_v_loss, mean_v_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T15:46:35.765436Z",
     "start_time": "2023-05-15T15:46:35.748982Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adam optimizer with learning rate set with the value of the LR hyperparameter\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "# The cross-entropy loss function is commonly used for classification tasks\n",
    "loss_fn = CrossEntropyLoss()\n",
    "# Instantiation of the trainer\n",
    "oracle_trainer = OracleTrainer(model,loss_fn,optimizer,dl_train,dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "processing step 1 of 110\n",
      "processing step 2 of 110\n",
      "processing step 3 of 110\n",
      "processing step 4 of 110\n",
      "processing step 5 of 110\n",
      "processing step 6 of 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    stats = oracle_trainer.train(NUM_EPOCHS)\n",
    "except RuntimeError as e:\n",
    "    print(\"Runtime Exception...\")\n",
    "    torch.cuda.empty_cache()\n",
    "    raise e\n",
    "\n",
    "# Check if the directory exists, to save the statistics of the training\n",
    "output_dir = os.path.join(d_path, \"output\")\n",
    "if not os.path.exists(output_dir):\n",
    "    # If the path does not exists, create it\n",
    "    os.makedirs(output_dir)\n",
    "# Save the statistics in json format\n",
    "with open(os.path.join(output_dir, f\"loss_accuracy_{BATCH_SIZE}_{LR}_{NUM_EPOCHS}.json\", \"w\")) as loss_file:\n",
    "    data = {\n",
    "        **stats,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"num_epochs\": NUM_EPOCHS\n",
    "    }\n",
    "    json.dump(data, loss_file)\n",
    "# Close the file\n",
    "loss_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Save the statistics and the trained model\n",
    "\n",
    "Saves the statistics for future analysis, and the trained model for future use or improvements.\n",
    "Saving the model we save the values of all the weights. In other words, we create a snapshot of\n",
    "the state of the model, after the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(output_dir, \"tratto_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load the model\n",
    "\n",
    "Commands to load the model to future uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(output_dir, \"tratto_model.pt\")))\n",
    "# Put the model in eval mode to use it in predictions\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.10 (default, Mar 13 2023, 10:26:41) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
