import ast
import re
from functools import partial
from src.utils import utils
if utils.is_rapids_available():
    import cudf.pandas
    cudf.pandas.install()
    import pandas as pd
else:
    import pandas as pd
import numba as nb
import os
from typing import Type, Dict, List, Tuple, Callable
from pandas import DataFrame
import numpy as np
import torch
import random
import csv
from torch.utils.data import TensorDataset
from transformers import PreTrainedTokenizer
from src.types.ClassificationType import ClassificationType
from src.types.DatasetType import DatasetType
from src.types.TransformerType import TransformerType
from src.types.TrattoModelType import TrattoModelType
import gc
import xml.etree.ElementTree as ET
import xml.dom.minidom

class DataProcessor:
    """
    The *DataProcessor* class takes care of pre-processing the raw datasets to create the final tokenized datasets.
    The datasets generated by the class can be feed to the *Dataset* and *DataLoader* PyTorch classes to easily load
    the data and prepare the batches to feed the model.

    Parameters
    ----------
    train_dataset_path: str
        The path to the training dataset
    validation_dataset_path: str
        The path to the validation dataset
    tokenizer: Type[PreTrainedTokenizer]
        The instance of tokenizer used to tokenize the input dataset.
    transformerType: Type[TransformerType]
        The classificator type (encoder or decoder)
    classification_type: Type[ClassificationType]
        Category predictions or label predictions.
    tratto_model_type: Type[TrattoModelType]
        TokenClasses or tokenValues model.
    is_multitask: bool
        True if the model is a multitask model, False otherwise.
    pre_processing: bool
        True if the dataset must be pre-processed, False otherwise.

    Attributes
    ----------
    _train_dataset_path: str
        The path to the training dataset
    _validation_dataset_path: str
        The path to the validation dataset
    _tokenizer  : Type[PreTrainedTokenizer]
        The instance of tokenizer used to tokenize the input dataset.
    _classification_type: Type[ClassificationType]
        Category predictions or label predictions.
    _transformer_type: Type[TransformerType]
        The classificatorType (encoder or decoder).
    _tratto_model_type: Type[TrattoModelType]
        TokenClasses or tokenValues model.
        The number of folds, if cross-validation is performed (folds > 1). Set to 1 by default (no cross-validation).
    _tgt_column_name: str
        The name of the target column of the dataset
    _cache: Dict[str, Dict[str, List[str]]]
        The cache contains values computed while processing the dataset, to avoid to recompute them each time.
    _is_multitask: bool
        True if the model is a multitask model, False otherwise. Default False.
    _pre_processing: bool
        True if the dataset must be pre-processed, False otherwise.
    """

    def __init__(
            self,
            train_dataset_path: str,
            validation_dataset_path: str,
            tokenizer: PreTrainedTokenizer,
            transformerType: Type[TransformerType],
            classification_type: ClassificationType,
            tratto_model_type: Type[TrattoModelType],
            is_multitask: bool = False,
            pre_processing: bool = False
    ):
        self._train_dataset_path = train_dataset_path
        self._validation_dataset_path = validation_dataset_path
        self._tokenizer = tokenizer
        self._classification_type = classification_type
        self._transformer_type = transformerType
        self._tratto_model_type = tratto_model_type
        self._is_multitask = is_multitask
        self._pre_processing = pre_processing
        if pre_processing:
            if classification_type == ClassificationType.LABEL_PREDICTION:
                self._tgt_column_name = "label"
            else:
                if tratto_model_type == TrattoModelType.TOKEN_CLASSES:
                    self._tgt_column_name = "tokenClass"
                elif tratto_model_type == TrattoModelType.TOKEN_VALUES:
                    self._tgt_column_name = "token"
                else:
                    raise Exception(
                        f"Tratto model type {tratto_model_type} not allowed for classification type equals to CATEGORY_PREDICTION.")
        else:
            self._tgt_column_name = "tgt"
        self._tokenized_train_dataset = {
            "src": None,
            "mask": None,
            "tgt": None
        }
        self._tokenized_validation_dataset = {
            "src": None,
            "mask": None,
            "tgt": None
        }
        self._cache = {}

    @staticmethod
    @nb.njit
    def create_numba_dict(items):
        """
        The method creates a dictionary from a list of tuples, compatible with Numba library.
        This method is necessary to overcome the limitations of Numba library, that does not support
        standard Python dictionaries.

        Parameters
        ----------
        items

        Returns
        -------
        A dictionary compatible with Numba library.
        """
        return {k: v for k, v in items}

    @staticmethod
    def convert_values(numba_value_mappings, df):
        # Create an empty list to store the transformed values
        transformed_values = []

        # Iterate through each row in the DataFrame
        for x in df["tokenClassesSoFar"]:
            # Convert the string representation of a list to an actual list
            list_representation = ast.literal_eval(x)

            # Create an empty list to store the mapped values
            mapped_list = []

            # Iterate through each element in the list and map using the value_mappings dictionary
            for y in list_representation:
                mapped_list.append(numba_value_mappings[y])

            # Append the mapped list to the result list
            transformed_values.append(mapped_list)

        # Assign the result list to the "tokenClassesSoFar" column in the DataFrame
        return transformed_values

    def compute_weights(
            self,
            column_name: str,
            df_type: Type[DatasetType] = None,
            df: Type[DataFrame] = None
    ):
        """
        The method computes the weights to assign to each value of the list passed to the function, for the given
        dataset. The weights are used to assign a different importance to each value of the list (targets), in
        the computation of the loss of the classification task, when the dataset is imbalanced. By default, if the
        dataframe is not provided and the dataset type is not provided as well, the method uses the training dataframe.

        Parameters
        ----------
        column_name: str
            The name of the column of the dataset to process
        df_type: Type[DatasetType]
            The dataset type (DatasetType.TRAINING or DatasetType.VALIDATION). Default None.
        df: Type[DataFrame]
            The dataframe to process. Default None.

        Returns
        -------
        values_weights : List[float]
            A list containing the weights for each different value of the column within the dataset. The length of
            the list is equal to the number of unique values in the column processed.
        """
        if column_name is None:
            column_name = self._tgt_column_name
        # Assert arguments are legal
        assert df_type is None or df_type in [DatasetType.TRAINING,
                                              DatasetType.VALIDATION], f"Dataset type {df_type} not recognized."
        assert df_type is not None or (df_type is None and df is None), "Illegal arguments combination."
        if df_type is None:
            df_type = DatasetType.TRAINING
        if "weights" not in self._cache:
            self._cache["weights"] = {}
        if df_type.value not in self._cache["weights"]:
            self._cache["weights"][df_type.value] = {}
        if df_type is None or column_name not in self._cache["weights"][df_type.value]:
            # Get training dataframe, if df is not provided
            if df is None:
                if df_type == DatasetType.TRAINING:
                    df = self.get_train_dataframe()
                else:
                    df = self.get_validation_dataframe()
            # Assert the column name is in the dataframe
            assert column_name in df.columns, f"Column {column_name} not found in the dataset."
            # Get the list of unique labels and the count the occurrences of each class
            unique_classes, class_counts = np.unique(df[column_name], return_counts=True)
            # Calculate the inverse class frequencies
            total_samples = np.sum(class_counts)
            class_frequencies = class_counts / total_samples
            class_weights = 1.0 / class_frequencies
            # Normalize the class weights
            class_weights /= np.sum(class_weights)
            # Save weights on cache
            self._cache["weights"][df_type.value][column_name] = class_weights
        # Return the computed weights
        return self._cache["weights"][df_type.value][column_name]

    @staticmethod
    def generate_datapoint_input(
            row: Type[pd.Series],
            tratto_model_type: Type[TrattoModelType],
            is_multitask: bool = False
    ):
        """
        The method generates the input for the model, given a row of the dataframe.

        Parameters
        ----------
        row: pandas.Series
            The row of the dataframe to process.
        tratto_model_type: Type[TrattoModelType]
            The type of the tratto model (ORACLES, TOKEN_CLASSES, TOKEN_VALUES).
        is_multitask: bool
            True if the model is a multitask model, False otherwise. Default False.

        Returns
        -------
        The input for the model.
        """

        # Create the root element
        datapoint_tag = ET.Element("Datapoint")
        # Identify input type, if the tratto model type is multitask
        if is_multitask:
            # Create the child element
            input_type_tag = ET.SubElement(datapoint_tag, "InputType")
            input_type_tag.text = row["inputType"]
        # Shared tags
        javadoc_tag = ET.Element("JavadocTag")
        javadoc_tag.text = row["javadocTag"] if len(row["javadocTag"]) > 0 else "_|empty|_"
        oracle_type_tag = ET.Element( "OracleType")
        oracle_type_tag.text = row["oracleType"]
        methodJavadoc_tag = ET.Element( "MethodJavadoc")
        methodJavadoc_tag.text = row["methodJavadoc"] if len(row["methodJavadoc"]) > 0 else "_|empty|_"
        methodSourceCode_tag = ET.Element( "MethodSourceCode")
        methodSourceCode_tag.text = row["methodSourceCode"] if len(row["methodSourceCode"]) > 0 else "_|empty|_"
        # packageName_tag = ET.Element( "PackageName")
        # packageName_tag.text = row["packageName"]
        # className_tag = ET.Element( "ClassName")
        # className_tag.text = row["className"]
        # classJavadoc_tag = ET.Element( "ClassJavadoc")
        # classJavadoc_tag.text = row["classJavadoc"]
        # classSourceCode_tag = ET.Element( "ClassSourceCode")
        # classSourceCode_tag.text = row["classSourceCode"]
        if tratto_model_type in [TrattoModelType.TOKEN_CLASSES, TrattoModelType.TOKEN_VALUES]:
            oracleSoFar_tag = ET.Element( "OracleSoFar")
            oracleSoFar_tag.text = row["oracleSoFar"] if len(row["oracleSoFar"]) > 0 else "_|empty|_"
        # Token Classes tags
        if tratto_model_type == TrattoModelType.TOKEN_CLASSES:
            tokenClass_tag = ET.Element( "TokenClass")
            tokenClass_tag.text = row["tokenClass"]
            tokenClassesSoFar_tag = ET.Element( "TokenClassesSoFar")
            tokens_list = row["tokenClassesSoFar"][1:-1].split()
            for token in tokens_list:
                if token.strip() == "" and len(tokens_list) == 1:
                    tokenClassesSoFar_tag.text = "_|empty|_"
                    break
                option_element = ET.SubElement(tokenClassesSoFar_tag, "TokenClassAdded")
                option_element.text = token.strip()[1:-1]
            eligibleTokenClasses_tag = ET.Element( "EligibleTokenClasses")
            tokens_list = row["eligibleTokenClasses"][1:-1].split()
            for token in tokens_list:
                option_element = ET.SubElement(eligibleTokenClasses_tag, "TokenClassOption")
                option_element.text = token.strip()
        #Token Values tags
        if tratto_model_type == TrattoModelType.TOKEN_VALUES:
            token_tag = ET.Element( "Token")
            token_tag.text = row["token"]
            tokenInfo_tag = ET.Element( "TokenInfo")
            tokens_list = row["tokenInfo"][1:-1].split(',')
            for token in tokens_list:
                if token.strip() == "" and len(tokens_list) == 1:
                    tokenInfo_tag.text = "_|empty|_"
                    break
                info_element = ET.SubElement(tokenInfo_tag, "Info")
                info_element.text = token.strip()[1:-1]
        #eligibleTokens_tag = ET.Element( "EligibleTokens")
        #eligibleTokens_tag.text = row["eligibleTokens"]
        # Compose the XML tree
        if tratto_model_type == TrattoModelType.ORACLES:
            datapoint_tag.append(javadoc_tag)
            datapoint_tag.append(oracle_type_tag)
            datapoint_tag.append(methodSourceCode_tag)
            datapoint_tag.append(methodJavadoc_tag)
        if tratto_model_type == TrattoModelType.TOKEN_CLASSES:
            datapoint_tag.append(tokenClass_tag)
            datapoint_tag.append(oracleSoFar_tag)
            datapoint_tag.append(tokenClassesSoFar_tag)
            datapoint_tag.append(eligibleTokenClasses_tag)
            datapoint_tag.append(javadoc_tag)
            datapoint_tag.append(oracle_type_tag)
            datapoint_tag.append(methodSourceCode_tag)
            datapoint_tag.append(methodJavadoc_tag)
        if tratto_model_type == TrattoModelType.TOKEN_VALUES:
            datapoint_tag.append(token_tag)
            datapoint_tag.append(tokenInfo_tag)
            datapoint_tag.append(oracleSoFar_tag)
            datapoint_tag.append(javadoc_tag)
            datapoint_tag.append(oracle_type_tag)
            datapoint_tag.append(methodSourceCode_tag)
            datapoint_tag.append(methodJavadoc_tag)
        # Convert the XML tree to a string with indentation
        datapoint_xml_string = xml.dom.minidom.parseString(ET.tostring(datapoint_tag, encoding="utf-8")).toprettyxml(indent="\t")
        # Clean the XML string
        datapoint_xml_string = re.sub(r'<MethodSourceCode>([\s\S]*?)</MethodSourceCode>', lambda match: "<MethodSourceCode>" + ('\n' + match.group(1)).replace('\n','\n\t\t') + "\n\t</MethodSourceCode>", datapoint_xml_string)
        datapoint_xml_string = re.sub(r'<MethodJavadoc>([\s\S]*?)</MethodJavadoc>', lambda match: "<MethodJavadoc>" + ('\n' + match.group(1)).replace('\n','\n\t') + "\n\t</MethodJavadoc>", datapoint_xml_string)
        datapoint_xml_string = re.sub(r'<JavadocTag>([\s\S]*?)</JavadocTag>', lambda match: "<JavadocTag>" + ('\n' + match.group(1)).replace('\n','\n\t\t') + "\n\t</JavadocTag>", datapoint_xml_string)
        datapoint_xml_string = datapoint_xml_string[len("<?xml version=\"1.0\" ?>\n"):] \
            .replace("&lt;", "<") \
            .replace("&gt;", ">") \
            .replace("&amp;", "&") \
            .replace("_|empty|_", "") \
            .replace("<JavadocTag>\n\t\t\n\t</JavadocTag>", "<JavadocTag></JavadocTag>") \
            .replace("<MethodSourceCode>\n\t\n\t</MethodSourceCode>", "<MethodSourceCode></MethodSourceCode>") \
            .replace("<MethodJavadoc>\n\t\n\t</MethodJavadoc>", "<MethodJavadoc></MethodJavadoc>") \
            .replace("<Info>", "<Info>\n\t\t\t") \
            .replace("</Info>", "\n\t\t</Info>") \
            .replace("<TokenClassOption>", "<TokenClassOption>\n\t\t\t") \
            .replace("</TokenClassOption>", "\n\t\t</TokenClassOption>") \
            .replace("<OracleType>", "<OracleType>\n\t\t") \
            .replace("</OracleType>", "\n\t</OracleType>") \
            .replace("<Token>", "<Token>\n\t\t") \
            .replace("</Token>", "\n\t</Token>") \
            .replace("<OracleSoFar>", "<OracleSoFar>\n\t\t") \
            .replace("</OracleSoFar>", "\n\t</OracleSoFar>") \
            .replace("<OracleSoFar>\n\t\t\n\t</OracleSoFar>", "<OracleSoFar></OracleSoFar>") \
            #.replace("<JavadocTag>", "<JavadocTag>\n\t\t") \
            #.replace("</JavadocTag>", "\n\t</JavadocTag>") \

            #re.sub(r'<TokenInfo>\s+</TokenInfo>', '<TokenInfo>\s+</TokenInfo>', datapoint_xml_string)
            # Return the XML string
        return datapoint_xml_string

    def get_encoder_labels_ids(
            self,
            column_name: str = None,
            df_type: Type[DatasetType] = None,
            df: Type[DataFrame] = None
    ):
        """
        The method computes the dictionary of the target labels for a given column name of a given dataframe, where
        each key represents the name of a target label, while the corresponding value is a numerical identifier
        representing the index of the one-shot vector representing the label, with value equals to 1.0. If the dataframe
        provided is None, the method uses the training dataframe.

        Parameters
        ----------
        column_name: str
            The name of the column of the dataset to process. Default None.
        df_type: Type[DatasetType]
            The dataset type (DatasetType.TRAINING or DatasetType.VALIDATION). Default None.
        df: Type[DataFrame]
            The dataframe to process. Default None.

        Returns
        -------
        The dictionary of labels. The keys strings representing the name of the corresponding target label, while the
        values are strings representing the name of the corresponding target label.
        """
        if column_name is None:
            column_name = self._tgt_column_name
        # Assert arguments are legal
        assert df_type is None or df_type in [DatasetType.TRAINING, DatasetType.VALIDATION], f"Dataset type {df_type} not recognized."
        assert df_type is not None or (df_type is None and df is None), "Illegal arguments combination."

        if df_type is None and df is None:
            df_type = DatasetType.TRAINING
            df = self.get_train_dataframe()
        if "encoder_labels_ids" not in self._cache:
            self._cache["encoder_labels_ids"] = {}
        if df_type is None or df_type.value not in self._cache["encoder_labels_ids"]:
            self._cache["encoder_labels_ids"][df_type.value] = {}
        if column_name not in self._cache["encoder_labels_ids"][df_type.value]:
            self._cache["encoder_labels_ids"][df_type.value][column_name] = {k: i for i, k in self.get_encoder_ids_labels(column_name, df_type, df).items()}
        # Return the computed dictionary
        return self._cache["encoder_labels_ids"][df_type.value][column_name]

    def get_encoder_ids_labels(
            self,
            column_name: str = None,
            df_type: Type[DatasetType] = None,
            df: Type[DataFrame] = None
    ):
        """
        The method computes the dictionary of target labels for a given column name of a given dataframe, where each
        value represents the name of a target label, while the corresponding key element is a numerical identifier
        representing the index of the one-shot vector representing the label, with value equals to 1.0. If the column
        name provided is None, the method uses the default target column name (self._tgt_column_name). If the dataframe
        provided is None, the method uses the training dataframe.

        Parameters
        ----------
        column_name: str
            The name of the column of the dataset to process. Default None.
        df_type: Type[DatasetType]
            The dataset type (DatasetType.TRAINING or DatasetType.VALIDATION). Default None.
        df: Type[DataFrame]
            The dataframe to process. Default None.

        Returns
        -------
        The dictionary of labels. The keys are numerical identifiers (int), while the values are strings representing the
        name of the corresponding target label.
        """
        if column_name is None:
            column_name = self._tgt_column_name
        # Assert arguments are legal
        assert df_type is None or df_type in [DatasetType.TRAINING, DatasetType.VALIDATION], f"Dataset type {df_type} not recognized."
        assert df_type is not None or (df_type is None and df is None), "Illegal arguments combination."

        if df_type is None:
            df_type = DatasetType.TRAINING
        if "encoder_ids_labels" not in self._cache:
            self._cache["encoder_ids_labels"] = {}
        if df_type.value not in self._cache["encoder_ids_labels"]:
            self._cache["encoder_ids_labels"][df_type.value] = {}
        if column_name not in self._cache["encoder_ids_labels"][df_type.value]:
            if df is None:
                if df_type == DatasetType.TRAINING:
                    df = self.get_train_dataframe()
                else:
                    df = self.get_validation_dataframe()
            # Assert the column name is in the dataframe
            assert column_name in df.columns, f"Column {column_name} not found in the dataset."
            self._cache["encoder_ids_labels"][df_type.value][column_name] = {i: str(k) for i, k in enumerate(sorted(list(df[column_name].unique())))}
        # Return the computed dictionary
        return self._cache["encoder_ids_labels"][df_type.value][column_name]

    def get_num_labels(
            self,
            column_name: str = None,
            df_type: Type[DatasetType] = None,
            df: Type[DataFrame] = None
    ):
        """
        Get the number of unique values of the target labels, for a given column name of the given dataframe.
        If the column name provided is None, the method uses the default target column name (self._tgt_column_name).

        Parameters
        ----------
        column_name: str
            The name of the column of the dataset to process. Default None.
        df_type: Type[DatasetType]
            The dataset type (DatasetType.TRAINING or DatasetType.VALIDATION). Default None.
        df: Type[DataFrame]
            The dataframe to process. Default None.

        Returns
        -------
        An integer representing the number of unique values of labels.
        """
        if column_name is None:
            column_name = self._tgt_column_name
        # Assert arguments are legal
        assert df_type is None or df_type in [DatasetType.TRAINING, DatasetType.VALIDATION], f"Dataset type {df_type} not recognized."
        assert df_type is not None or (df_type is None and df is None), "Illegal arguments combination."
        return len(self.get_encoder_ids_labels(column_name, df_type, df))

    @staticmethod
    def get_token_classes_value_mappings():
        """
        The method loads the value mappings to substitute token classes default names.
        """
        _, value_mappings = utils.import_json(
            os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                '..',
                'resources',
                'token_classes_values_mappings.json'
            )
        )
        return value_mappings

    @staticmethod
    def get_token_classes_ignore_values():
        """
        The method loads the token classes to ignore by the token values model.
        """
        _, ignore_values = utils.import_json(
            os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                '..',
                'resources',
                'token_classes_ignore_value_mappings.json'
            )
        )
        return ignore_values

    def get_tokenized_dataset(
            self,
            d_type: Type[DatasetType]
    ):
        """
        The method returns the processed tokenized (training or validation) dataset.

        Parameters
        ----------
        d_type: DatasetType
            The dataset type that the method must return
                - DatasetType.TRAINING for the training dataset
                - DatasetType.VALIDATION for the validation dataset

        Returns
        -------
        t_t_dataset : TensorDataset
            A PyTorch TensorDataset composed of four tensors stack:
                - the first tensor stack representing the stack of tokenized input
                  datapoints of the selected dataset
                - the second tensor stack representing the stack of attention masks
                  (each index corresponding to the index of the input datapoints in
                  the first tensor)
                - the third tensor stack representing the list of expected target
                  outputs (each index corresponding to the index of the input
                  datapoints in the first tensor)
        """
        # Select the tokenized dataset
        if d_type == DatasetType.TRAINING:
            t_dataset = (
                self._tokenized_train_dataset["src"],
                self._tokenized_train_dataset["mask"],
                self._tokenized_train_dataset["tgt"]
            )
        elif d_type == DatasetType.VALIDATION:
            t_dataset = (
                self._tokenized_validation_dataset["src"],
                self._tokenized_validation_dataset["mask"],
                self._tokenized_validation_dataset["tgt"]
            )
        else:
            raise Exception(f"Unrecognized DataType value: {d_type}")
        # Generate the tokenized tensor dataset, from the list of tokenized datapoints
        t_t_dataset = TensorDataset(*t_dataset)
        # Return the dataset
        return t_t_dataset

    def get_train_dataframe(self):
        """
        The method returns the training dataset as a pandas dataframe.

        Returns
        -------
        The training dataset as a pandas dataframe.
        """
        # Get the dataset to process
        t_df = self.load_dataset_as_dataframe(self._train_dataset_path, self._tratto_model_type, self._pre_processing)
        # Apply values replacement if the dataset must be pre-processed
        if self._pre_processing:
            if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
                # Get token classes value mappings
                value_mappings = self.get_token_classes_value_mappings()
                # Replace the values in the DataFrame column
                t_df['tokenClass'] = t_df['tokenClass'].replace(value_mappings)
                if utils.is_rapids_available():
                    # Create a dictionary compatible with Numba library
                    numba_value_mappings = self.create_numba_dict(tuple(value_mappings.items()))
                    # Map token classes so far to new values and transform it from array to string
                    t_df["tokenClassesSoFar"] = self.convert_values(numba_value_mappings, t_df)
                else:
                    # Map token classes so far to new values and transform it from array to string
                    t_df["tokenClassesSoFar"] = t_df["tokenClassesSoFar"].apply(lambda x: [value_mappings[y] for y in x])
        return t_df

    def get_validation_dataframe(self):
        """
        The method returns the validation dataset as a pandas dataframe.

        Returns
        -------
        The validation dataset as a pandas dataframe.
        """
        # Get the dataset to process
        v_df = self.load_dataset_as_dataframe(self._validation_dataset_path, self._tratto_model_type, self._pre_processing)
        # Apply values replacement if the dataset must be pre-processed
        if self._pre_processing:
            if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
                # Get token classes value mappings
                value_mappings = self.get_token_classes_value_mappings()
                # Replace the values in the DataFrame column
                v_df['tokenClass'] = v_df['tokenClass'].replace(value_mappings)
                if utils.is_rapids_available():
                    # Create a dictionary compatible with Numba library
                    numba_value_mappings = self.create_numba_dict(tuple(value_mappings.items()))
                    # Map token classes so far to new values and transform it from array to string
                    v_df["tokenClassesSoFar"] = self.convert_values(numba_value_mappings, v_df)
                else:
                    # Map token classes so far to new values and transform it from array to string
                    v_df["tokenClassesSoFar"] = v_df["tokenClassesSoFar"].apply(lambda x: [value_mappings[y] for y in x])
        return v_df

    @staticmethod
    def load_dataset_as_dataframe(
            dataset_path: str,
            tratto_model_type: Type[TrattoModelType],
            pre_processing: bool = False
    ):
        """
            The method loads the dataset from specified paths,and generate the corresponding pandas DataFrames.
            The method collects the partial dataframes from a collection of files representing a dataset split
            in files, and merge them into a single dataframe.
            Drops empty `label` column and fills null values with empty strings.

            Parameters
            ----------
            dataset_path: str
                The path to the dataset
            tratto_model_type: Type[TrattoModelType]
                The type of Tratto model considered (tokenClasses, tokenValues, or oracles).
            pre_processing: bool
                True if the dataset must be pre-processed, False otherwise.

            Returns
            -------
            t_df: pandas.DataFrame
                A pandas DataFrame representation of the train dataset
            v_df: pandas.DataFrame
                A pandas DataFrame representation of the validation dataset
        """
        # List of partial dataframes
        partial_dfs = []
        # Collects partial dataframes from oracles
        for file_name in os.listdir(dataset_path):
            if utils.is_rapids_available() or not pre_processing:
                df = pd.read_csv(os.path.join(dataset_path, file_name))
            else:
                df = pd.read_json(os.path.join(dataset_path, file_name))
            if pre_processing:
                if tratto_model_type == TrattoModelType.ORACLES:
                    # Generate labels for oracles model and map column 'id` to `oracleId`
                    if len(df) > 0:
                        # The oracles model must return True if the oracle is not empty (there is an oracle to generate), False otherwise
                        # An empty oracle is an oracle with ";" as value
                        df["label"] = df["oracle"].apply(lambda o: "False" if o == ";" else "True")
                        # Map id into oracleId
                        if "oracleId" not in df.columns:
                            if "id" in df.columns:
                                df.rename(columns={'id': 'oracleId'}, inplace=True)
                            else:
                                raise Exception("Both columns 'id' and 'oracleId' not found in the dataset.")
            partial_dfs.append(df)
        # Concat the partial dataframes into a single dataframe
        df_dataset = pd.concat(partial_dfs)
        # Reset the index of the dataframe
        df_dataset.reset_index(drop=True, inplace=True)
        # Return the dataset in the form of pandas dataframe
        return df_dataset

    def pre_processing(
            self,
            save: bool = False
    ):
        """
        The method pre-processes the loaded dataset that will be used to train and validate the model.
        It generates the final tokenized dataset, ready to be used to train and validate the model.
        The method also saves the pre-processed dataset, if required.

        Parameters
        ----------
        save: bool
            True if the pre-processed dataset must be saved, False otherwise.
        """
        if not self._pre_processing and save:
            print("Pre-processing is not enabled, but save is set to True. The dataset will not be saved since the dataset will be considered already pre-processed.")
        # Get train and validation dataframes
        t_df = self.get_train_dataframe()
        v_df = self.get_validation_dataframe()
        # Apply pre-processing if required
        if self._pre_processing:
            # Map empty cells to empty strings
            t_df.fillna('', inplace=True)
            v_df.fillna('', inplace=True)
            # Get the dictionary of encoder label ids
            labels_ids_dict = self.get_encoder_labels_ids(self._tgt_column_name, df_type=DatasetType.TRAINING, df=t_df)
            # Pre-process the dataset, according to the Tratto model
            if self._tratto_model_type == TrattoModelType.ORACLES:
                # Specify the type of each column in the dataset
                df_types = {
                    'label': 'str',
                    'oracleId': 'int64',
                    'oracle': 'str',
                    'oracleType': 'str',
                    'javadocTag': 'str',
                    'methodJavadoc': 'str',
                    'methodSourceCode': 'str'
                }
                if self._is_multitask:
                    t_df['inputType'] = 'oracle'
                    v_df['inputType'] = 'oracle'
                    df_types['inputType'] = 'str'
                # Set the type of each column in the dataset
                t_df = t_df.astype(df_types)
                v_df = v_df.astype(df_types)
                # Balance dataset
                # t_df = self._balance_dataframe(t_df)
                # Define the new order of columns
                new_columns_order = [
                    'javadocTag', 'oracleType', 'methodSourceCode', 'methodJavadoc', 'oracleId', 'oracle', 'label'
                ]
                if self._is_multitask:
                    new_columns_order.insert(0, 'inputType')
                # Reindex the DataFrame with the new order
                t_df = t_df.reindex(columns=new_columns_order)
                v_df = v_df.reindex(columns=new_columns_order)
                # Get the list of target values from the dataframe
                if self._transformer_type == TransformerType.DECODER:
                    t_tgt = t_df[self._tgt_column_name].values.tolist()
                    v_tgt = v_df[self._tgt_column_name].values.tolist()
                else:
                    t_tgt = list(map(lambda t: labels_ids_dict[t], t_df[self._tgt_column_name].values.tolist()))
                    v_tgt = list(map(lambda t: labels_ids_dict[t], v_df[self._tgt_column_name].values.tolist()))
                # Drop oracle column
                t_df.drop(['oracle'], axis=1, inplace=True)
                v_df.drop(['oracle'], axis=1, inplace=True)
            else:
                # Specify the type of each column in the dataset
                df_types = {
                    'label': 'str',
                    'oracleId': 'int64',
                    'oracleType': 'str',
                    'packageName': 'str',
                    'className': 'str',
                    'javadocTag': 'str',
                    'methodJavadoc': 'str',
                    'methodSourceCode': 'str',
                    'oracleSoFar': 'str',
                    'token': 'str',
                    'tokenClass': 'str',
                    'tokenInfo': 'str'
                }
                if self._is_multitask:
                    input_type = 'token_class' if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES else 'tokenValue'
                    t_df['inputType'] = input_type
                    v_df['inputType'] = input_type
                    df_types['inputType'] = 'str'
                # Set the type of each column in the dataset
                t_df = t_df.astype(df_types)
                v_df = v_df.astype(df_types)
                # Enrich vocabulary
                # self.enrich_vocabulary(self._tokenizer, self._tratto_model_type)
                # Balance dataset
                # t_df = self._balance_dataframe(t_df)
                # Pre-process the dataset, according to the Tratto model considered (tokenClasses or tokenValues).
                if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
                    # Map token classes so far to new values and transform it from array to string
                    t_df = self.compute_tokenClassesSoFar(t_df)
                    v_df = self.compute_tokenClassesSoFar(v_df)
                    # Compute eligible token classes
                    t_df = self.compute_eligible_token_classes(t_df)
                    v_df = self.compute_eligible_token_classes(v_df)
                    # Define the new order of columns
                    new_columns_order = [
                        'tokenClass', 'oracleSoFar', 'tokenClassesSoFar', 'eligibleTokenClasses',
                        'javadocTag', 'oracleType', 'packageName', 'className', 'methodSourceCode', 'methodJavadoc',
                        'classJavadoc', 'classSourceCode', 'oracleId', 'label', 'token', 'tokenInfo', 'projectName'
                    ]
                    if self._is_multitask:
                        new_columns_order.insert(0, 'inputType')
                    # Reindex the DataFrame with the new order
                    t_df = t_df.reindex(columns=new_columns_order)
                    v_df = v_df.reindex(columns=new_columns_order)
                else:
                    # Compute eligible token values
                    #t_df = self.compute_eligible_token_values(t_df)
                    #v_df = self.compute_eligible_token_values(v_df)
                    # Define the new order of columns
                    new_columns_order = [
                        'token', 'oracleSoFar',
                        #'eligibleTokens',
                        'javadocTag', 'oracleType',
                        'packageName', 'className', 'methodSourceCode', 'methodJavadoc', 'tokenInfo',
                        'classJavadoc', 'classSourceCode', 'oracleId', 'label', 'tokenClass', 'projectName'
                    ]
                    if self._is_multitask:
                        new_columns_order.insert(0, 'inputType')
                    # Reindex the DataFrame with the new order
                    t_df = t_df.reindex(columns=new_columns_order)
                    #t_df = t_df.drop(['eligibleTokens'], axis=1)
                    v_df = v_df.reindex(columns=new_columns_order)
                    #v_df = v_df.drop(['eligibleTokens'], axis=1)
                # Keep only the instance with label 'True', for each combination of oracleId and oracleSoFar, if the model
                # predicts the categories (tokenClasses or tokenValues)
                if self._classification_type == ClassificationType.CATEGORY_PREDICTION:
                    t_df = t_df[t_df['label'] == 'True']
                    v_df = v_df[v_df['label'] == 'True']
                # Get the list of target values from the dataframe
                if self._transformer_type == TransformerType.DECODER:
                    t_tgt = t_df[self._tgt_column_name].values.tolist()
                    v_tgt = v_df[self._tgt_column_name].values.tolist()
                else:
                    t_tgt = list(map(lambda t: labels_ids_dict[t], t_df[self._tgt_column_name].values.tolist()))
                    v_tgt = list(map(lambda t: labels_ids_dict[t], v_df[self._tgt_column_name].values.tolist()))
                # If the model predicts token classes, remove the token values from the input, else remove
                # the token classes from the input
                if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
                    t_df = t_df.drop(['token', 'tokenInfo'], axis=1)
                    v_df = v_df.drop(['token', 'tokenInfo'], axis=1)
                    # Remove the tokenClass column if the model will predict the tokenClass as target
                    if self._classification_type == ClassificationType.CATEGORY_PREDICTION:
                        t_df = t_df.drop(['tokenClass'], axis=1)
                        v_df = v_df.drop(['tokenClass'], axis=1)
                else:
                    t_df = t_df.drop(['tokenClass'], axis=1)
                    v_df = v_df.drop(['tokenClass'], axis=1)
                    # Remove the token column if the model will predict the token as target
                    if self._classification_type == ClassificationType.CATEGORY_PREDICTION:
                        t_df = t_df.drop(['token'], axis=1)
                        v_df = v_df.drop(['token'], axis=1)
                    # TODO: Check if tokenInfo must be removed
                    # Remove the tokenInfo column if the classificator is a decoder and the model will predict the token as target
                    if self._transformer_type == TransformerType.DECODER and self._classification_type == ClassificationType.CATEGORY_PREDICTION:
                        t_df = t_df.drop(['tokenInfo'], axis=1)
                        v_df = v_df.drop(['tokenInfo'], axis=1)
            # Delete the tgt labels from the input dataset, and others irrelevant columns
            if self._tratto_model_type == TrattoModelType.ORACLES:
                t_df.drop(['label', 'oracleId'], axis=1, inplace=True)
                v_df.drop(['label', 'oracleId'], axis=1, inplace=True)
            else:
                t_df.drop(['label', 'oracleId', 'classJavadoc', 'classSourceCode', 'projectName'], axis=1, inplace=True)
                v_df.drop(['label', 'oracleId', 'classJavadoc', 'classSourceCode', 'projectName'], axis=1, inplace=True)
            # Bind parameter values to input generator function
            binded_generate_datapoint_input = partial(
                self.generate_datapoint_input,
                tratto_model_type=self._tratto_model_type,
                is_multitask=self._is_multitask
            )
            # Generate input strings for training and validation datasets
            t_src = self.concat_src(t_df, self._tokenizer, binded_generate_datapoint_input)
            v_src = self.concat_src(v_df, self. _tokenizer, binded_generate_datapoint_input)
            # Release memory to reduce consumptions with large datasets
            del t_df
            del v_df
            gc.collect()
            # Save src and tgt datasets
            if save:
                filename = f'src_tgt_{self._tratto_model_type.lower()}_{self._classification_type.lower()}_{self._transformer_type.lower()}.csv'
                file_dir_path = os.path.join(
                    os.path.dirname(os.path.abspath(__file__)),
                    '..',
                    '..',
                    'dataset',
                    'pre_processed_datasets',
                    self._tratto_model_type.lower(),
                    self._classification_type.lower(),
                    self._transformer_type.lower()
                )
                if not os.path.exists(os.path.join(file_dir_path, "train")):
                    os.makedirs(os.path.join(file_dir_path, "train"))
                if not os.path.exists(os.path.join(file_dir_path, "validation")):
                    os.makedirs(os.path.join(file_dir_path, "validation"))
                with open(os.path.join(file_dir_path, "train", f"train_{filename}"), 'w', newline='') as csvfile:
                    csv_writer = csv.writer(csvfile)
                    csv_writer.writerow(['src', 'tgt'])
                    csv_writer.writerows(zip(t_src, t_tgt))
                with open(os.path.join(file_dir_path, "validation", f"validation_{filename}"), 'w', newline='') as csvfile:
                    csv_writer = csv.writer(csvfile)
                    csv_writer.writerow(['src', 'tgt'])
                    csv_writer.writerows(zip(v_src, v_tgt))
        else:
            if t_df['tgt'].dtype == bool:
                t_df['tgt'] = t_df['tgt'].astype(str)
            if v_df['tgt'].dtype == bool:
                v_df['tgt'] = v_df['tgt'].astype(str)
            t_src = t_df['src'].values.tolist()
            t_tgt = t_df['tgt'].values.tolist()
            v_src = v_df['src'].values.tolist()
            v_tgt = v_df['tgt'].values.tolist()
        # Generate tensors of tokenized input and target datapoints
        t_t_train = self.tokenize_dataset(t_src, t_tgt, self._tokenizer, self._transformer_type)
        t_t_validation = self.tokenize_dataset(v_src, v_tgt, self._tokenizer, self._transformer_type)
        # Release memory to reduce consumptions with large datasets
        del t_src
        del v_src
        del t_tgt
        del v_tgt
        gc.collect()
        # Save the tokenized datasets
        self._tokenized_train_dataset["src"] = t_t_train[0]
        self._tokenized_train_dataset["mask"] = t_t_train[1]
        self._tokenized_train_dataset["tgt"] = t_t_train[2]
        self._tokenized_validation_dataset["src"] = t_t_validation[0]
        self._tokenized_validation_dataset["mask"] = t_t_validation[1]
        self._tokenized_validation_dataset["tgt"] = t_t_validation[2]

    @staticmethod
    def _balance_dataframe(df):
        # Extract empty oracles from dataframe
        df_empty_semicolon_true = df[
            (df['oracleSoFar'] == '') &
            (df['token'] == ';') &
            (df['label'] == True)
        ]
        # Extract non-empty oracles from dataframe
        df_not_empty_semicolon_true = df[
            (df['oracleSoFar'].str.strip() != '') |
            (df['token'] != ';') |
            (df['label'] != True)
        ]
        # Compute the size of the two dataframes
        df_empty_size = len(df_empty_semicolon_true)
        df_not_empty_size = len(df_not_empty_semicolon_true)
        # Check if the two dataframes are imbalanced
        if df_empty_size > 0 and (0.8 < df_not_empty_size / df_empty_size < 1.2):
            # Start building the balanced DataFrame
            major_df = df_empty_semicolon_true if df_empty_size > df_not_empty_size else df_not_empty_semicolon_true
            minor_df = df_empty_semicolon_true if df_empty_size < df_not_empty_size else df_not_empty_semicolon_true
            balanced_df = minor_df
            # Group the major dataframe by 'oracleId'
            groups = major_df.groupby('oracleId')
            rows_counter = 0
            # Iterate over the groups and randomly select groups until the desired number of rows is reached
            for name, group in groups:
                rows_counter += len(group)
                # Add all rows of the group to the balanced DataFrame
                balanced_df = pd.concat([balanced_df, group])
                # Check if adding the group's rows would exceed the desired number of rows
                if len(balanced_df) + len(group) > 2 * len(minor_df):
                    break
            print(f"Balanced dataset: {len(minor_df)} minor df + {rows_counter} major df = {len(balanced_df)}")
            # Reset the index of the balanced DataFrame
            balanced_df = balanced_df.reset_index(drop=True)
            return balanced_df
        return df

    @staticmethod
    def tokenize_dataset(
            inputs: List[str],
            targets: List[List[float]],
            tokenizer: Type[PreTrainedTokenizer],
            transformer_type: Type[TransformerType]
    ):
        """
        The method tokenizes the input and target datapoints passed to the function

        Parameters
        ----------
        inputs: List[str]
            The list of input datapoints to tokenize
        targets: List[List[float]]
            The list of target datapoints to tokenize
        tokenizer: Type[PreTrainedTokenizer]
            The tokenizer to use to tokenize the datapoints
        transformer_type: Type[TransformerType]
            The type of transformer (encoder or decoder)

        Returns
        -------
        tokenized_batches : Tuple[List[int],List[float],List[float]]
            A tuple of 3 lists, where the first list represents the tokenized strings of the input datapoints of the
            dataset, the second list is the tensor of the corresponding attention-masks, the third element is the tensor
            of the corresponding targets.
        """
        # The following method generatse a dictionary with two keys:
        #
        #   t_src_dict = {
        #       "input_ids": [[t_i_1_1,...,t_i_1_n],...,[t_i_k_1,...,t_i_k_n]],
        #       "attention_mask": [[m_1_1,...,m_k_n],...,[m_k_1,...,m_k_n]]
        #   }
        #
        # where each element in the *input_ids* list is a list of tokenized words
        # (the words of an input datapoint), while each element in the *attention
        # masks* is the corresponding mask vector to distinguish the real tokens
        # from the padding tokens. In the example, t_i_x_y is the y tokenized word
        # of the input datapoint x, and m_x_y is a boolean value that states if
        # the token y is a real word or a padding token.
        #
        t_src_dict = tokenizer.batch_encode_plus(
            inputs,
            max_length=512,
            truncation=True,
            padding=True,
            return_tensors="pt"
        )
        if transformer_type == TransformerType.DECODER:
            t_tgt_dict = tokenizer.batch_encode_plus(
                targets,
                max_length=8,
                truncation=True,
                padding=True,
                return_tensors="pt"
            )
        # Transform the list into a tensor stack
        #
        #   t_src_dict['input_ids'] = [[t_i_1_1,...,t_i_1_n],...,[t_i_k_1,...,t_i_k_n]]
        #
        #   t_inputs = tensor([
        #           [t_i_1_1,...,t_i_1_n],
        #                   ...
        #           [t_i_k_1,...,t_i_k_n]
        #   ])
        #
        # this is the structure accepted by the DataLoader, to process the dataset
        t_inputs = torch.stack([ids.clone().detach() for ids in t_src_dict['input_ids']])
        t_inputs_attention_masks = torch.stack([mask.clone().detach() for mask in t_src_dict['attention_mask']])
        if transformer_type == TransformerType.DECODER:
            t_targets = torch.stack([ids.clone().detach() for ids in t_tgt_dict['input_ids']])
        else:
            t_targets = torch.stack([torch.tensor(t) for t in targets])
        # Generate the tokenized dataset
        tokenized_dataset = (t_inputs, t_inputs_attention_masks, t_targets)
        # Return the tokenized dataset
        return tokenized_dataset

    @staticmethod
    def compute_tokenClassesSoFar(
            df: Type[DataFrame]
    ):
        """
        The method computes the token classes added so far to a given oracle, for each datapoint of the dataframe
        passed to the function.

        Parameters
        ----------
        df: Type[DataFrame]
            The dataframe to process

        Returns
        -------
        df: Type[DataFrame]
            The dataframe with the token classes so far computed
        """
        # Map token classes so far to new values and transform it from array to string
        mapped_tokenClassesSoFar_series = df["tokenClassesSoFar"].apply(
            lambda x: "[ " + " ".join(random.sample([y for y in x], len(x))) + " ]")
        df.loc[:, 'tokenClassesSoFar'] = None
        # Set type of new dataframe column
        df.loc[:, 'tokenClassesSoFar'] = df['tokenClassesSoFar'].astype('str')
        # Assign the new values to the dataframe
        df.loc[:, 'tokenClassesSoFar'] = mapped_tokenClassesSoFar_series
        return df

    @staticmethod
    def compute_eligible_token_classes(
            df: Type[DataFrame]
    ):
        """
        The method computes the eligible token classes for each datapoint of the dataframe passed to the function.
        Parameters
        ----------
        df: Type[DataFrame]
            The dataframe to process

        Returns
        -------
        df: Type[DataFrame]
            The dataframe with the eligible token classes computed
        """
        # Compute eligible token classes
        df_eligibleTokenClasses = df.groupby(['oracleId', 'oracleSoFar'])['tokenClass'].unique().to_frame()
        df_eligibleTokenClasses = df_eligibleTokenClasses.rename(columns={'tokenClass': 'eligibleTokenClasses'})
        df = pd.merge(df, df_eligibleTokenClasses, on=['oracleId', 'oracleSoFar']).reset_index()
        df["eligibleTokenClasses"] = df["eligibleTokenClasses"].apply(
            lambda x: "[ " + " ".join(random.sample(list(x), len(x))) + " ]")
        # Set type of new dataframe column
        df['eligibleTokenClasses'] = df['eligibleTokenClasses'].astype('str')
        return df

    @staticmethod
    def compute_eligible_token_values(df):
        """
        The method computes the eligible token values for each datapoint of the dataframe passed to the function.

        Parameters
        ----------
        df: Type[DataFrame]
            The dataframe to process

        Returns
        -------
        df: Type[DataFrame]
            The dataframe with the eligible token values computed
        """
        df_eligibleTokens = df.groupby(['oracleId', 'oracleSoFar'])['token'].unique().to_frame()
        df_eligibleTokens = df_eligibleTokens.rename(columns={'token': 'eligibleTokens'})
        df = pd.merge(df, df_eligibleTokens, on=['oracleId', 'oracleSoFar']).reset_index()
        df["eligibleTokens"] = df["eligibleTokens"].apply(
            lambda x: "[ " + " ".join(random.sample(list(x), len(x))) + " ]")
        # Set type of new dataframe column
        df['eligibleTokens'] = df['eligibleTokens'].astype('str')
        return df

    @staticmethod
    def concat_src(
            df: Type[DataFrame],
            tokenizer: Type[PreTrainedTokenizer],
            lambda_function: Callable = None
    ):
        """
        Maps each row of the given dataframe, with multiple columns, to a row with a single column containing the
        concatenation of the strings of each original column.
        By default, if no callable is passed to the method, it concatenates the columns using a token as a separator (</s>).
        For example:

          1. Given the first row of the dataset:

                projectName                                 "commons-collections4-4.1"
                packageName                          "org.apache.commons.collections4"
                className                                                    "Equator"
                javadocTag                "@return whether the two objects are equal."
                methodJavadoc      "/**\n     * Evaluates the two arguments for th..."
                methodSourceCode                         "boolean equate(T o1, T o2);"
                classJavadoc       "/**\n * An equation function, which determines..."
                oracleSoFar                                                         ""
                token                                                              "("
                tokenClass                                        "OpeningParenthesis"
                tokenInfo                                                           ""
                notes                                                               ""

          2. The statement gets the values of each column in an array (row.values)

                ["commons-collections4-4.1","org.apache.commons.collections4",...,"",""]

          3. The join method concatenates all the values in the array into a string,
             using a special token (the *sep_token*) as separator.

                commons-collections4-4.1<s>org.apache.commons.collections4<s>...<s>OpeningParenthesis<s><s>

        The result of step (3) represents the content of the unique column of the new map row. The process is repeated
        for each row in the diven dataframe.

        Otherwise, if the lambda_function parameter is not None, the method uses the lambda function to build the input,
        for each row of the dataframe df. For example, a lambda function can create a string representing an xml tree,
        instead of a concatenated string.

        Finally, the dataframe is transformed into a numpy list where each element corresponds to a row of the new
        dataframe, i.e. a string representing the concatenation of the values of the original columns.

        Parameters
        ----------
        df: Type[DataFrame]
            The dataframe to process
        tokenizer: Type[PreTrainedTokenizer]
            The tokenizer to use to tokenize the input strings
        lambda_function: Callable
            A lambda function to apply to each row to generate a custom input. The parameter is optional, None by default.

        Returns
        -------
        rows_list: List[str]
            A list of strings, where each string represents the concatenation of the values of the original columns
            of the dataframe.
        """
        if lambda_function is not None:
            df = df.apply(lambda row: lambda_function(row=row), axis=1)
        else:
            df = df.apply(lambda row: tokenizer.sep_token.join(row.values), axis=1)
        rows_list = df.to_numpy().tolist()
        return rows_list

    @staticmethod
    def enrich_vocabulary(
            tokenizer: Type[PreTrainedTokenizer],
            tratto_model_type: Type[TrattoModelType],
            multitask: bool = False
    ):
        """
        The method enriches the vocabulary of the tokenizer with the words of the token classes.

        Parameters
        ----------
        tokenizer: Type[PreTrainedTokenizer]
            The tokenizer to enrich.
        tratto_model_type: Type[TrattoModelType]
            The type of Tratto model considered (tokenClasses, tokenValues, or oracles).

        Returns
        -------

        """
        # Map token class names
        value_mappings = DataProcessor.get_token_classes_value_mappings()
        # If the model is for the token values consider only a subset of words
        # (the other ones will never appear whitin the dataset)
        if tratto_model_type == TrattoModelType.TOKEN_VALUES and not multitask:
            ignore_values = DataProcessor.get_token_classes_ignore_values()
        vocab = tokenizer.get_vocab()
        for old_word, new_word in value_mappings.items():
            if (not multitask) and tratto_model_type == TrattoModelType.TOKEN_VALUES and old_word in ignore_values:
                continue
            for new_sub_word in new_word.split("_"):
                if not new_sub_word in vocab.keys():
                    tokenizer.add_tokens([new_sub_word])
