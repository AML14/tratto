import os
from typing import Type, Dict, List, Tuple
import pandas as pd
import numpy as np
import torch
from torch.utils.data import TensorDataset
from sklearn.model_selection import train_test_split, StratifiedKFold
from transformers import PreTrainedTokenizer
from src.types.ClassificationType import ClassificationType
from src.types.DatasetType import DatasetType
from src.types.TrattoModelType import TrattoModelType


class DataProcessor:
    """
    The *DataProcessor* class takes care of pre-processing the raw datasets to create
    the final tokenized datasets.
    The datasets generated by the class can be feed to the *Dataset* and *DataLoader*
    PyTorch classes to easily load the data and prepare the batches to feed the model.

    Parameters
    ----------
    d_path : str
        The path to the dataset.
    test_ratio: float
        The ratio of the original dataset reserved for testing (acceptable range of values: from 0.0 to 1.0).
    tokenizer  : Type[PreTrainedTokenizer]
        The instance of tokenizer used to tokenize the input dataset.
    folds: int
        The number of folds, if cross-validation is performed (folds > 1). Set to 1 by default (no cross-validation)
    classification_type: Type[ClassificationType]
        Category predictions or label predictions.
    tratto_model_type: Type[TrattoModelType]
        TokenClasses or tokenValues model.

    Attributes
    ----------
    d_path : str
        The path to the dataset.
    test_ratio: float
        The ratio of the original dataset reserved for testing (acceptable range of values: from 0.0 to 1.0).
    tokenizer  : Type[PreTrainedTokenizer]
        The instance of tokenizer used to tokenize the input dataset.
    classification_type: Type[ClassificationType]
        Category predictions or label predictions.
    tratto_model_type: Type[TrattoModelType]
        TokenClasses or tokenValues model.
    folds: int
        The number of folds, if cross-validation is performed (folds > 1). Set to 1 by default (no cross-validation)
    src : List[str]
        The list of input datapoints (concatenated strings, with separator token). This source list is used to train and
        validate the model. Initially set to None.
    tgt : List[int]
        The list of expected class values (in one-shot vector format) for each datapoint in input to the model.
        This target list is used to train and validate the model. Initially set to None.
    tgt_map : Dict[str,List[int]]
        The dictionary is composed of all the unique target values, in string format (as key), and the corresponding
        one-shot vector representation (as value).
    src_test : List[str]
        The list of input datapoints (concatenated strings, with separator token). This source list is used to test the
        model. Initially set to None.
    tgt_test : List[int]
        The list of expected values (in one-shot vector format) for a subset of datapoints in input to the model.
        This target list is used to test the model. Initially set to None.
    processed_dataset: Dict[str,List[List[Union[str,int]]]]
        Dictionary of the processed dataset:
            - train: contains a list (each element representing a fold) of datasets for the training phase, after the
              original dataset has been splitted (according to *validation_ratio* value). and tokenized.
            - val: contains a list (each element representing a fold) of datasets for the validation phase, after the
              original dataset has been splitted (according to *validation_ratio* value). and tokenized.
            - test: contains a list representing the dataset for the testing phase, after it has been tokenized.
            - train: contains a list (each element representing a fold) of datasets for the training phase, after the
              original dataset has been splitted (according to *validation_ratio* value)
            - val: contains a list (each element representing a fold) of datasets for the validation phase, after the
              original dataset has been splitted (according to *validation_ratio* value), and tokenized.
            - test: contains a list representing the dataset for the testing phase, after it has been tokenized.
    """
    def __init__(
            self,
            d_path: str,
            test_ratio: float,
            tokenizer: PreTrainedTokenizer,
            classification_type: ClassificationType,
            tratto_model_type: Type[TrattoModelType],
            folds: int = 1
    ):
        self._d_path = d_path
        self._tokenizer = tokenizer
        self._test_ratio = test_ratio
        self._df_dataset = self._load_dataset(d_path)
        self._src = None
        self._tgt = None
        self._tgt_map = {}
        self._src_test = None
        self._tgt_test = None
        self._folds = folds
        self._classification_type = classification_type
        self._tratto_model_type = tratto_model_type
        self._processed_dataset = {
            "train": [],
            "val": [],
            "test": [],
            "t_train": [],
            "t_val": [],
            "t_test": []
        }

    def compute_weights(
            self,
            column_name: str
    ):
        """
        The method computes the weights to assign to each value of the list passed to the function.
        The weights are used to assign a different importance to each value of the list (targets), in the computation
        of the loss of the classification task, when the dataset is imbalanced.

        Parameters
        ----------
        column_name: str
            The name of the column of the dataset to process
        
        Returns
        -------
        weights : List[float]
            A list containing the values of the weights for each different value within the column of the dataset.
            The length of the list is equal to the number of unique values in the column processed.
        """
        # Get the list of unique labels and the count the occurrences of each class
        unique_classes, class_counts = np.unique(self._df_dataset[column_name], return_counts=True)
        # Calculate the inverse class frequencies
        total_samples = np.sum(class_counts)
        class_frequencies = class_counts / total_samples
        class_weights = 1.0 / class_frequencies
        # Normalize the class weights
        class_weights /= np.sum(class_weights)
        # Return the computed weights
        return class_weights

    def get_ids_labels(self):
        """
        The method computes the dictionary of target labels of the classification model, where each value represents the
        name of a target label, while the corresponding key element is a numerical identifier representing the index of
        the one-shot vector representing the label, with value equals to 1.0.

        Returns
        -------
        The dictionary of labels. The keys are numerical identifiers (int), while the values are strings representing the
        name of the corresponding target label. The dictionary is empty if the dataset has not been processed yet.
        """
        ids_labels_dict = { i:k for i,k in enumerate(self._tgt_map.keys()) }
        # If the dictionary is empty prints a warning
        if not bool(ids_labels_dict):
            print("The ids-label dictionary is empty. It may be that the dataset has not been processed yet.")
        return ids_labels_dict

    def get_num_labels(self):
        """
        Get the number of unique values of target labels.

        Returns
        -------
        An integer representing the number of unique values of labels (0 if the dataset has not been processed yet).
        """
        num_labels = len(self.get_ids_labels())
        # If the dictionary is empty print a warning
        if num_labels == 0:
            print("The number of labels is 0. It may be that the dataset has not been processed yet.")
        return num_labels


    def get_classes_ids(self):
        """
        The method computes the dictionary of target classes of the classification model, where the key is the name of a
        class, while the value element is a numerical identifier representing the codification of a corresponding class.

        Returns
        -------
        The dictionary of classes. The keys are strings representing the name of a class (int), while the values are the
        corresponding numeric codification.
        """
        if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
            return {k: i for i, k in enumerate(self._df_dataset["tokenClass"].unique())}
        else:
            return {k: i for i, k in enumerate(self._df_dataset["token"].unique())}

    def get_ids_classes(self):
        """
        The method computes the dictionary of classes of the classification model, a numerical identifier representing
        the index of the one-shot vector representing the class while the value element is the name of the corresponding
        target class.

        Returns
        -------
        The dictionary of classes. The keys are numeric identifiers (integer) representing the codification value of a
        class (int), while the values are the name of the corresponding target class.
        """
        return { i:k for k,i in self.get_classes_ids().items()}

    def get_tgt_classes_size(self):
        """
        The method computes the number of classes of the classification model.

        Returns
        -------
        The number of target classes of the classification model.
        """
        classes_size = len(self._tgt_map.keys())
        if classes_size == 0:
            print("[Warn] - classes size is 0")
        return classes_size

    def get_src(self):
        """
        The method returns a copy of the list of input datapoints used for training and validation.

        Returns
        -------
        src : List[str]
            A copy of the list of input datapoints used for training and validation.
        """
        src = [*self._src]
        return src

    def get_tgt(self):
        """
        The method returns a copy of the list of targets associated to the input datapoints used for training and
        validation.

        Returns
        -------
        src : List[str]
            A copy of the list of input datapoints for training and validation.
        """
        src = [*self._src]
        return src

    def get_src_test(self):
        """
        The method returns a copy of the list of input datapoints used for testing.

        Returns
        -------
        src : List[str]
            A copy of the list of input datapoints for testing.
        """
        src_test = [*self._src_test]
        return src_test

    def get_tgt_test(self):
        """
        The method returns a copy of the list of the targets associated to the input datapoints used for testing.

        Returns
        -------
        tgt : List[str]
            A copy of the list of targets associated to the input datapoints for testing.
        """
        tgt_test = [*self._tgt_test]
        return tgt_test

    def get_tokenized_dataset(
            self,
            d_type: Type[DatasetType],
            fold_idx: int = 0
    ):
        """
        The method returns the processed tokenized (training or validation) dataset of the requested fold.
        If cross-validation is not performed the first and only fold is returned (by default, the fold index is equal
        to 0, therefore no crossa-validation is considered).

        Parameters
        ----------
        d_type: DatasetType
            The dataset type that the method must return
                - DatasetType.TRAINING for the training dataset
                - DatasetType.VALIDATION for the validation dataset
                - DatasetType.TEST for the test dataset
        fold_idx: int
            The index of the fold (0 by default, if no cross-validation is performed)

        Returns
        -------
        t_t_dataset : TensorDataset
            A PyTorch TensorDataset composed of four tensors stack:
                - the first tensor stack representing the stack of tokenized input
                  datapoints of the selected dataset
                - the second tensor stack representing the stack of attention masks
                  (each index corresponding to the index of the input datapoints in
                  the first tensor)
                - the third tensor stack representing the list of expected target
                  outputs (each index corresponding to the index of the input
                  datapoints in the first tensor)
                - the fourth tensor stack representing the list of expected target
                  labels (each index corresponding to the index of the input
                  datapoints in the first tensor)
        """
        # Select the tokenized dataset
        if d_type == DatasetType.TRAINING:
            t_dataset = self._processed_dataset["t_train"][fold_idx]
        elif d_type == DatasetType.VALIDATION:
            t_dataset = self._processed_dataset["t_val"][fold_idx]
        elif d_type == DatasetType.TEST:
            t_dataset = self._processed_dataset["t_test"]
        else:
            raise Exception(f"Unrecognized DataType value: {d_type}")
        # Generate the tensor dataset from the list of tokenized datapoints
        t_t_dataset = TensorDataset(*t_dataset)
        # Return the dataset
        return t_t_dataset
        
    def map_column_values_to_one_shot_vectors(
            self,
            column_name: str
    ):
        """
        The method creates a dictionary that maps the unique values of a column of the dataset, to the corresponding
        one-shot vector representations.

        Parameters
        ----------
        column_name: str
            The name of the column of the dataset from which to map the values into the one-shot vector representations
        
        Returns
        -------
        mapping: Dict[Union[str,int],List[int]]
            The dictionary that maps the unique values of the column of the dataset, to the corresponding one-shot
            vector representations. Each key represents one of the unique values of the considered column.
        """
        # Get unique values
        unique_values = np.unique(self._df_dataset[column_name])
        # Create a dictionary to map string values to their corresponding vector
        mapping = {}
        # Populate the dictionary and generate the one-shot vectors.
        for i, value in enumerate(unique_values):
            vector = np.zeros(len(unique_values))
            vector[i] = 1.0
            mapping[value] = list(vector)
        # Return the dictionary
        return mapping


    def processing(self):
        """ 
        The method processes the original datasets to generate the training, validation, and test datasets to train and 
        test the model. If the attribute {@code self.folds} is greater than 1, the datasets are generated to perform 
        cross-validation (A number of sets equal to {@code self.folds} are generated). Otherwise, a single set of datasets 
        is generated.
        
        """
        # Create the cross-validation splitter, if the attribute folds is greater than 1
        if self._folds > 1:
            cross_validation = StratifiedKFold(n_splits=self._n_split, shuffle=True, random_state=42)
            print(f"        Generating {self._n_split} folds for cross-validation.")
            for fold, (t_fold_indices, v_fold_indices) in enumerate(cross_validation.split(self._src, np.array([np.array(dp) for dp in self._tgt]))):
                print(f"            Processing fold {fold + 1}.")
                # Split the dataset into training and validation source and target sets for the current fold
                t_src_fold_data = [self._src[i] for i in t_fold_indices]
                t_tgt_fold_data = [self._tgt[i] for i in t_fold_indices]
                v_src_fold_data = [self._src[i] for i in v_fold_indices]
                v_tgt_fold_data = [self._tgt[i] for i in v_fold_indices]
                t_fold_dataset  = (t_src_fold_data, t_tgt_fold_data)
                v_fold_dataset  = (v_src_fold_data, v_tgt_fold_data)
                # Tokenize training and validation datasets of the current fold
                t_t_fold_dataset = self._tokenize_dataset(t_fold_dataset)
                t_v_fold_dataset = self._tokenize_dataset(v_fold_dataset)
                # Append datasets of the current fold to the corresponding training and validation processes datasets
                self._processed_dataset["train"].append(t_fold_dataset)
                self._processed_dataset["val"].append(v_fold_dataset)
                # Append tokenized datasets of the current fold to the corresponding training and validation processes 
                # tokenized datasets
                self._processed_dataset["t_train"].append(t_t_fold_dataset)
                self._processed_dataset["t_val"].append(t_v_fold_dataset)
        else:
            # Split the original dataset in training and test sets
            t_src_data, v_src_data, t_tgt_data, v_tgt_data = train_test_split(self._src, self._tgt, test_size=self._test_ratio, stratify=self._tgt)
            # Generate training and validation sets
            t_dataset = (t_src_data, t_tgt_data)
            v_dataset = (v_src_data, v_tgt_data)
            # Tokenize training and validation datasets of the current fold
            t_t_dataset = self._tokenize_dataset(t_dataset)
            t_v_dataset = self._tokenize_dataset(v_dataset)
            # Append datasets of the current fold to the corresponding training and validation processes datasets
            self._processed_dataset["train"].append(t_dataset)
            self._processed_dataset["val"].append(v_dataset)
            # Append tokenized datasets of the current fold to the corresponding training and validation processes 
            # tokenized datasets
            self._processed_dataset["t_train"].append(t_t_dataset)
            self._processed_dataset["t_val"].append(t_v_dataset)
        # Generate test dataset
        test_dataset = (self._src_test, self._tgt_test)
        # Assign the test dataset to the processed datasets
        self._processed_dataset["test"] = (self._src_test, self._tgt_test)
        # Assign the tokenized test dataset to the processed tokenized datasets
        self._processed_dataset["t_test"] = self._tokenize_dataset(test_dataset)
        

    def pre_processing(self):
        """
        The method pre-processes the loaded dataset that will be used to train and test the model.
        """
        # Drop column id (it is not relevant for training the model)
        self._df_dataset = self._df_dataset.drop(['id'], axis=1)
        # Map empty cells to empty strings
        self._df_dataset.fillna('', inplace=True)
        # Specify the type of each column in the dataset
        self._df_dataset = self._df_dataset.astype({
            'label': 'str',
            'oracleId': 'int64',
            'oracleType': 'string',
            'projectName': 'string',
            'packageName': 'string',
            'className': 'string',
            'javadocTag': 'string',
            'methodJavadoc': 'string',
            'methodSourceCode': 'string',
            'classJavadoc': 'string',
            'classSourceCode': 'string',
            'oracleSoFar': 'string',
            'token': 'string',
            'tokenClass': 'string',
            'tokenInfo': 'string'
        })
        # Pre-process the dataset, according to the Tratto model considered (tokenClasses or tokenValues).
        if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
            #tokenClassesDict = self.get_classes_ids()
            self._df_dataset["tokenClassesSoFar"] = self._df_dataset["tokenClassesSoFar"].apply(lambda x: "[" + " ".join(x) + "]")
            #self._df_dataset["tokenClass"] = self._df_dataset["tokenClass"].apply(lambda x: str(tokenClassesDict[x]))
            df_eligibleTokenClasses = self._df_dataset.groupby(['oracleId', 'oracleSoFar'])['tokenClass'].unique().to_frame()
            df_eligibleTokenClasses = df_eligibleTokenClasses.rename(columns={'tokenClass': 'eligibleTokenClasses'})
            self._df_dataset = pd.merge(self._df_dataset, df_eligibleTokenClasses,on=['oracleId', 'oracleSoFar']).reset_index()
            self._df_dataset["eligibleTokenClasses"] = self._df_dataset["eligibleTokenClasses"].apply(lambda x: "[" + ",".join(x) + "]")
            self._df_dataset['eligibleTokenClasses'] = self._df_dataset['eligibleTokenClasses'].astype('string')
            self._df_dataset['tokenClass'] = self._df_dataset['tokenClass'].astype('string')
            self._df_dataset['tokenClassesSoFar'] = self._df_dataset['tokenClassesSoFar'].astype('string')
            # Define the new order of columns
            new_columns_order = [
                'token',
                'tokenInfo',
                'tokenClass',
                'oracleSoFar',
                'tokenClassesSoFar',
                'javadocTag',
                'oracleType',
                'packageName',
                'className',
                'methodJavadoc',
                'methodSourceCode',
                'classJavadoc',
                'classSourceCode',
                'projectName',
                'oracleId',
                'label'
            ]
            # Reindex the DataFrame with the new order
            self._df_dataset = self._df_dataset.reindex(columns=new_columns_order)
        else:
            # Define the new order of columns
            new_columns_order = [
                'tokenClass',
                'token',
                'tokenInfo',
                'oracleSoFar',
                'javadocTag',
                'oracleType',
                'projectName',
                'packageName',
                'className',
                'methodJavadoc',
                'methodSourceCode',
                'classJavadoc',
                'classSourceCode',
                'oracleId',
                'label'
            ]
            # Reindex the DataFrame with the new order
            self._df_dataset = self._df_dataset.reindex(columns=new_columns_order)

        if self._classification_type == ClassificationType.CATEGORY_PREDICTION:
            self._df_dataset = self._df_dataset[self._df_dataset['label'] == 'True']

        # Delete the tgt labels from the input dataset, and others less relevant columns
        df_src = self._df_dataset.drop(['label','oracleId','projectName','classJavadoc','classSourceCode'], axis=1)
        # If the model predicts token classes, remove the token values and the token info from the input, else remove
        # the token classes from the input
        if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
            df_src = df_src.drop(['token','tokenInfo'], axis=1)
            if self._classification_type == ClassificationType.CATEGORY_PREDICTION:
                df_src = df_src.drop(['tokenClass'], axis=1)
        else:
            df_src = df_src.drop(['tokenClass'], axis=1)
            if self._classification_type == ClassificationType.CATEGORY_PREDICTION:
                df_src = df_src.drop(['token'], axis=1)

        # The apply function maps each row of the src dataset with multiple columns, to
        # a row with a single column containing the concatenation of the strings of each
        # original column, using a token as a separator.
        #
        # For example:
        #
        #   1. Given the first row of the dataset:
        #
        #         projectName                                 "commons-collections4-4.1"
        #         packageName                          "org.apache.commons.collections4"
        #         className                                                    "Equator"
        #         javadocTag                "@return whether the two objects are equal."
        #         methodJavadoc      "/**\n     * Evaluates the two arguments for th..."
        #         methodSourceCode                         "boolean equate(T o1, T o2);"
        #         classJavadoc       "/**\n * An equation function, which determines..."
        #         oracleSoFar                                                         ""
        #         token                                                              "("
        #         tokenClass                                        "OpeningParenthesis"
        #         tokenInfo                                                           ""
        #         notes                                                               ""
        #
        #   2. The statement gets the values of each column in an array (row.values)
        #
        #         ["commons-collections4-4.1","org.apache.commons.collections4",...,"",""]
        #
        #   3. The join method concatenates all the values in the array into a string,
        #      using a special token (the *sep_token*) as separator.
        #
        #         commons-collections4-4.1<s>org.apache.commons.collections4<s>...<s>OpeningParenthesis<s><s>
        #
        # The result of step (3) represents the content of the unique column of the new
        # map row. The process is repeated for each row in the src dataset.
        df_src_concat = df_src.apply(lambda row: self._tokenizer.sep_token.join(row.values), axis=1)

        # The pandas dataframe is transformed in a list of strings: each string is an input to the model
        src = df_src_concat.to_numpy().tolist()
        # Get the list of target values from the dataframe
        if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
            tgt = self._df_dataset["tokenClass"].values if self._classification_type == ClassificationType.CATEGORY_PREDICTION else self._df_dataset["label"].values
        else:
            tgt = self._df_dataset["token"].values if self._classification_type == ClassificationType.CATEGORY_PREDICTION else self._df_dataset["label"].values
        # Split the dataset into training and test sets with stratified sampling (given imbalanced dataset), based on target classes
        self._src, self._src_test, self._tgt, self._tgt_test = train_test_split(src, tgt, test_size=self._test_ratio, stratify=tgt)
        # Generate the mapping of the target column unique values to the corresponding one-shot representations
        if self._tratto_model_type == TrattoModelType.TOKEN_CLASSES:
            self._tgt_map = self.map_column_values_to_one_shot_vectors("tokenClass" if self._classification_type == ClassificationType.CATEGORY_PREDICTION else "label")
        else:
            self._tgt_map = self.map_column_values_to_one_shot_vectors("token" if self._classification_type == ClassificationType.CATEGORY_PREDICTION else "label")

    def _load_dataset(
            self,
            d_path: str
    ):
        """
            The method applies the dataset preprocessing. It loads dataset from
            specified `d_path`. Drops empty `label` column and fills null values
            with empty strings.

            Parameters
            ----------
            d_path: str
                The path to the dataset

            Returns
            -------
            df_dataset: pandas.DataFrame
                A pandas DataFrame representation of the dataset
        """
        # list of partial dataframes
        dfs = []

        # datasets path
        oracles_dataset = os.path.join(d_path)
        # collects partial dataframes from oracles
        for file_name in os.listdir(oracles_dataset)[:100]:
            df = pd.read_json(os.path.join(oracles_dataset,  file_name))
            dfs.append(df)
        df_dataset = pd.concat(dfs)
        return df_dataset

    def _tokenize_dataset(
            self,
            datapoints: Tuple[List[str],List[List[float]]]
    ):
        """
        The method tokenizes the input and target datapoints passed to the function

        Parameters
        ----------
        datapoints: List[Tuple[str,Union[str,float]]]
            A list of datapoints  (tuples input and targets).

        Returns
        -------
        tokenized_batches : Tuple[List[int],List[float],List[float],List[Union[str,float]]]
            A tuple of 4 lists, where the first list represents the tokenized strings of the input datapoints of the dataset,
            the second list is the tensor of the corresponding attention-masks, the third element is the
            tensor of the corresponding targets, and the fourth element is the tensor of the target labels.
        """
        # Extracts the inputs datapoints from the dataset
        inputs = datapoints[0]
        # Extracts the corresponding targets datapoints from the dataset
        targets = datapoints[1]
        # Tokenize the inputs datapoints
        # The method generate a dictionary with two keys:
        #
        #   t_src_dict = {
        #       "input_ids": [[t_i_1_1,...,t_i_1_n],...,[t_i_k_1,...,t_i_k_n]],
        #       "attention_mask": [[m_1_1,...,m_k_n],...,[m_k_1,...,m_k_n]]
        #   }
        #
        # where each element in the *input_ids* list is a list of tokenized words
        # (the words of an input datapoint), while each element in the *attention
        # masks* is the corresponding mask vector to distinguish the real tokens
        # from the padding tokens. In the example, t_i_x_y is the y tokenized word
        # of the input datapoint x, and m_x_y is a boolean value that states if
        # the token y is a real word or a padding token
        #
        t_src_dict = self._tokenizer(
          inputs,
          padding='max_length',
          truncation=True
        )
        # Transform the list into a tensor stack
        #
        #   t_src_dict['input_ids'] = [[t_i_1_1,...,t_i_1_n],...,[t_i_k_1,...,t_i_k_n]]
        #
        #   t_inputs = tensor([
        #           [t_i_1_1,...,t_i_1_n],
        #                   ...
        #           [t_i_k_1,...,t_i_k_n]
        #   ])
        #
        # this is the structure accepted by the DataLoader, to process the dataset
        # Transform the list into a tensor stack
        t_inputs = torch.stack([ids.clone().detach() for ids in t_src_dict['input_ids']])
        t_attention_masks = torch.stack([mask.clone().detach() for mask in t_src_dict['attention_mask']])
        # Map targets value into one-shot vectors
        targets_one_shot = list(map(lambda t: self._tgt_map[t], targets))
        # Transform the targets into a tensor list
        targets_tensor = torch.tensor(targets_one_shot)
        # Keep track of labels
        if self._classification_type == ClassificationType.CATEGORY_PREDICTION:
            targets_labels = targets
        else:
            targets_labels = list(map(lambda i: i.split(self._tokenizer.sep_token)[0], inputs))
        classifier_classes_ids = self.get_classes_ids()
        targets_labels_tensor = torch.tensor(list(map(lambda l: classifier_classes_ids[l], targets_labels)))
        # Generate the tokenized dataset
        tokenized_dataset = (t_inputs, t_attention_masks, targets_tensor, targets_labels_tensor)
        # Return the tokenized dataset
        return tokenized_dataset